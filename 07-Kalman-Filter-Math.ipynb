{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Table of Contents](./table_of_contents.ipynb)\n",
    "\n",
    "# Kalman Filter Math\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#format the book\n",
    "import book_format\n",
    "book_format.set_style()\n",
    "\n",
    "If you've gotten this far I hope that you are thinking that the Kalman filter's fearsome reputation is somewhat undeserved. Sure, I hand waved some equations away, but I hope implementation has been fairly straightforward for you. The underlying concept is quite straightforward - take two measurements, or a measurement and a prediction, and choose the output to be somewhere between the two. If you believe the measurement more your guess will be closer to the measurement, and if you believe the prediction is more accurate your guess will lie closer to it. That's not rocket science (little joke - it is exactly this math that got Apollo to the moon and back!). \n",
    "\n",
    "To be honest I have been choosing my problems carefully. For an arbitrary problem designing the Kalman filter matrices can be extremely difficult. I haven't been *too tricky*, though. Equations like Newton's equations of motion can be trivially computed for Kalman filter applications, and they make up the bulk of the kind of problems that we want to solve. \n",
    "\n",
    "I have illustrated the concepts with code and reasoning, not math. But there are topics that do require more mathematics than I have used so far. This chapter presents the math that you will need for the rest of the book.\n",
    "\n",
    "## Modeling a Dynamic System\n",
    "\n",
    "A *dynamic system* is a physical system whose state (position, temperature, etc) evolves over time. Calculus is the math of changing values, so we use differential equations to model dynamic systems. Some systems cannot be modeled with differential equations, but we will not encounter those in this book.\n",
    "\n",
    "Modeling dynamic systems is properly the topic of several college courses. To an extent there is no substitute for a few semesters of ordinary and partial differential equations followed by a graduate course in control system theory. If you are a hobbyist, or trying to solve one very specific filtering problem at work you probably do not have the time and/or inclination to devote a year or more to that education.\n",
    "\n",
    "Fortunately, I can present enough of the theory to allow us to create the system equations for many different Kalman filters. My goal is to get you to the stage where you can read a publication and understand it well enough to implement the algorithms. The background math is deep, but in practice we end up using a few simple techniques. \n",
    "\n",
    "This is the longest section of pure math in this book. You will need to master everything in this section to understand the Extended Kalman filter (EKF), the most common nonlinear filter. I do cover more modern filters that do not require as much of this math. You can choose to skim now, and come back to this if you decide to learn the EKF.\n",
    "\n",
    "We need to start by understanding the underlying equations and assumptions that the Kalman filter uses. We are trying to model real world phenomena, so what do we have to consider?\n",
    "\n",
    "Each physical system has a process. For example, a car traveling at a certain velocity goes so far in a fixed amount of time, and its velocity varies as a function of its acceleration. We describe that behavior with the well known Newtonian equations that we learned in high school.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v&=at\\\\\n",
    "x &= \\frac{1}{2}at^2 + v_0t + x_0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Once we learned calculus we saw them in this form:\n",
    "\n",
    "$$ \\mathbf v = \\frac{d \\mathbf x}{d t}, \n",
    "\\quad \\mathbf a = \\frac{d \\mathbf v}{d t} = \\frac{d^2 \\mathbf x}{d t^2}\n",
    "$$\n",
    "\n",
    "A typical automobile tracking problem would have you compute the distance traveled given a constant velocity or acceleration, as we did in previous chapters. But, of course we know this is not all that is happening. No car travels on a perfect road. There are bumps, wind drag, and hills that raise and lower the speed. The suspension is a mechanical system with friction and imperfect springs.\n",
    "\n",
    "Perfectly modeling a system is impossible except for the most trivial problems. We are forced to make a simplification. At any time $t$ we say that the true state (such as the position of our car) is the predicted value from the imperfect model plus some unknown *process noise*:\n",
    "\n",
    "$$\n",
    "x(t) = x_{pred}(t) + noise(t)\n",
    "$$\n",
    "\n",
    "This is not meant to imply that $noise(t)$ is a function that we can derive analytically. It is merely a statement of fact - we can always describe the true value as the predicted value  plus the process noise. \"Noise\" does not imply random events. If we are tracking a thrown ball in the atmosphere, and our model assumes the ball is in a vacuum, then the effect of air drag is process noise in this context.\n",
    "\n",
    "In the next section we will learn techniques to convert a set of higher order differential equations into a set of first-order differential equations.  After the conversion the model of the system without noise is:\n",
    "\n",
    "$$ \\dot{\\mathbf x} = \\mathbf{Ax}$$\n",
    "\n",
    "$\\mathbf A$ is known as the *systems dynamics matrix* as it describes the dynamics of the system. Now we need to model the noise. We will call that $\\mathbf w$, and add it to the equation. \n",
    "\n",
    "$$ \\dot{\\mathbf x} = \\mathbf{Ax} + \\mathbf w$$\n",
    "\n",
    "$\\mathbf w$ may strike you as a poor choice for the name, but you will soon see that the Kalman filter assumes *white* noise.\n",
    "\n",
    "Finally, we need to consider any inputs into the system. We assume an input $\\mathbf u$, and that there exists a linear model that defines how that input changes the system. For example, pressing the accelerator in your car makes it accelerate, and gravity causes balls to fall. Both are control inputs. We will need a matrix $\\mathbf B$ to convert $u$ into the effect on the system. We add that into our equation:\n",
    "\n",
    "$$ \\dot{\\mathbf x} = \\mathbf{Ax} + \\mathbf{Bu} + \\mathbf{w}$$\n",
    "\n",
    "And that's it. That is one of the equations that Dr. Kalman set out to solve, and he found an optimal estimator if we assume certain properties of $\\mathbf w$.\n",
    "\n",
    "## State-Space Representation of Dynamic Systems\n",
    "\n",
    "We've derived the equation\n",
    "\n",
    "$$ \\dot{\\mathbf x} = \\mathbf{Ax}+ \\mathbf{Bu} + \\mathbf{w}$$\n",
    "\n",
    "However, we are not interested in the derivative of $\\mathbf x$, but in $\\mathbf x$ itself. Ignoring the noise for a moment, we want an equation that recursively finds the value of $\\mathbf x$ at time $t_k$ in terms of $\\mathbf x$ at time $t_{k-1}$:\n",
    "\n",
    "$$\\mathbf x(t_k) = \\mathbf F(\\Delta t)\\mathbf x(t_{k-1}) + \\mathbf B(t_k)\\mathbf u (t_k)$$\n",
    "\n",
    "Convention allows us to write $\\mathbf x(t_k)$ as $\\mathbf x_k$, which means the \n",
    "the value of $\\mathbf x$ at the $k^{th}$ value of $t$.\n",
    "\n",
    "$$\\mathbf x_k = \\mathbf{Fx}_{k-1} + \\mathbf B_k\\mathbf u_k$$\n",
    "\n",
    "$\\mathbf F$ is the familiar *state transition matrix*, named due to its ability to transition the state's value between discrete time steps. It is very similar to the system dynamics matrix $\\mathbf A$. The difference is that $\\mathbf A$ models a set of linear differential equations, and is continuous. $\\mathbf F$ is discrete, and represents a set of linear equations (not differential equations) which transitions $\\mathbf x_{k-1}$ to $\\mathbf x_k$ over a discrete time step $\\Delta t$. \n",
    "\n",
    "Finding this matrix is often quite difficult. The equation $\\dot x = v$ is the simplest possible differential equation and we trivially integrate it as:\n",
    "\n",
    "$$ \\int\\limits_{x_{k-1}}^{x_k}  \\mathrm{d}x = \\int\\limits_{0}^{\\Delta t} v\\, \\mathrm{d}t $$\n",
    "$$x_k-x_{k-1} = v \\Delta t$$\n",
    "$$x_k = v \\Delta t + x_{k-1}$$\n",
    "\n",
    "This equation is *recursive*: we compute the value of $x$ at time $k$ based on its value at time $k-1$. This recursive form enables us to represent the system (process model) in the form required by the Kalman filter:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbf x_k &= \\mathbf{Fx}_{k-1}  \\\\\n",
    "&= \\begin{bmatrix} 1 & \\Delta t \\\\ 0 & 1\\end{bmatrix}\n",
    "\\begin{bmatrix}x_{k-1} \\\\ \\dot x_{k-1}\\end{bmatrix}\n",
    "\\end{aligned}$$\n",
    "\n",
    "We can do that only because $\\dot x = v$ is simplest differential equation possible. Almost all other in physical systems result in more complicated differential equation which do not yield to this approach. \n",
    "\n",
    "*State-space* methods became popular around the time of the Apollo missions, largely due to the work of Dr. Kalman. The idea is simple. Model a system with a set of  $n^{th}$-order differential equations. Convert them into an equivalent set of first-order differential equations. Put them into the vector-matrix form used in the previous section: $\\dot{\\mathbf x} = \\mathbf{Ax} + \\mathbf{Bu}$. Once in this form we use of several techniques to convert these linear differential equations into the recursive equation:\n",
    "\n",
    "$$ \\mathbf x_k = \\mathbf{Fx}_{k-1} + \\mathbf B_k\\mathbf u_k$$\n",
    "\n",
    "Some books call the state transition matrix the *fundamental matrix*. Many use $\\mathbf \\Phi$ instead of $\\mathbf F$. Sources based heavily on control theory tend to use these forms.\n",
    "\n",
    "These are called *state-space* methods because we are expressing the solution of the differential equations in terms of the system state. \n",
    "\n",
    "### Forming First Order Equations from Higher Order Equations\n",
    "\n",
    "Many models of physical systems require second or higher order differential equations with control input $u$:\n",
    "\n",
    "$$a_n \\frac{d^ny}{dt^n} + a_{n-1} \\frac{d^{n-1}y}{dt^{n-1}} +  \\dots + a_2 \\frac{d^2y}{dt^2} + a_1 \\frac{dy}{dt} + a_0 = u$$\n",
    "\n",
    "State-space methods require first-order equations. Any higher order system of equations can be reduced to first-order by defining extra variables for the derivatives and then solving. \n",
    "\n",
    "\n",
    "Let's do an example. Given the system $\\ddot{x} - 6\\dot x + 9x = u$ find the equivalent first order equations. I've used the dot notation for the time derivatives for clarity.\n",
    "\n",
    "The first step is to isolate the highest order term onto one side of the equation.\n",
    "\n",
    "$$\\ddot{x} = 6\\dot x - 9x + u$$\n",
    "\n",
    "We define two new variables:\n",
    "\n",
    "$$\\begin{aligned} x_1(t) &= x \\\\\n",
    "x_2(t) &= \\dot x\n",
    "\\end{aligned}$$\n",
    "\n",
    "Now we will substitute these into the original equation and solve. The solution yields a set of first-order equations in terms of these new variables. It is conventional to drop the $(t)$ for notational convenience.\n",
    "\n",
    "We know that $\\dot x_1 = x_2$ and that $\\dot x_2 = \\ddot{x}$. Therefore\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\dot x_2 &= \\ddot{x} \\\\\n",
    "         &= 6\\dot x - 9x + u\\\\\n",
    "         &= 6x_2-9x_1 + u\n",
    "\\end{aligned}$$\n",
    "\n",
    "Therefore our first-order system of equations is\n",
    "\n",
    "$$\\begin{aligned}\\dot x_1 &= x_2 \\\\\n",
    "\\dot x_2 &= 6x_2-9x_1 + u\\end{aligned}$$\n",
    "\n",
    "If you practice this a bit you will become adept at it. Isolate the highest term, define a new variable and its derivatives, and then substitute.\n",
    "\n",
    "### ğŸ§  å¦‚ä½•å°†é«˜é˜¶å¾®åˆ†æ–¹ç¨‹è½¬åŒ–ä¸ºä¸€é˜¶å¾®åˆ†æ–¹ç¨‹ç»„ï¼ˆçŠ¶æ€ç©ºé—´å½¢å¼ï¼‰\n",
    "\n",
    "å¾ˆå¤šç‰©ç†ç³»ç»Ÿï¼ˆå¦‚æ§åˆ¶ç³»ç»Ÿã€ç”µè·¯ç³»ç»Ÿï¼‰ç”±äºŒé˜¶æˆ–æ›´é«˜é˜¶çš„å¾®åˆ†æ–¹ç¨‹æè¿°ã€‚ä½†ä¸ºäº†ä½¿ç”¨çŠ¶æ€ç©ºé—´æ–¹æ³•å»ºæ¨¡åˆ†æï¼Œæˆ‘ä»¬éœ€è¦å°†å®ƒä»¬è½¬æ¢ä¸ºä¸€é˜¶å¾®åˆ†æ–¹ç¨‹ç»„ã€‚\n",
    "\n",
    "### ğŸ“ ä¸¾ä¾‹è¯´æ˜\n",
    "\n",
    "ç»™å®šç³»ç»Ÿï¼š\n",
    "\n",
    "$$\n",
    "\\ddot{x} - 6\\dot{x} + 9x = u\n",
    "$$\n",
    "\n",
    "### âœ… ç¬¬ä¸€æ­¥ï¼šå°†æœ€é«˜é˜¶å¯¼æ•°ï¼ˆ$\\ddot{x}$ï¼‰å•ç‹¬åˆ—åœ¨ç­‰å¼å·¦è¾¹\n",
    "\n",
    "$$\n",
    "\\ddot{x} = 6\\dot{x} - 9x + u\n",
    "$$\n",
    "\n",
    "### âœ… ç¬¬äºŒæ­¥ï¼šå®šä¹‰æ–°å˜é‡\n",
    "\n",
    "å°†åŸå§‹å˜é‡åŠå…¶å¯¼æ•°æ›¿æ¢æˆæ–°å˜é‡ï¼š\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x_1 &= x \\\\\n",
    "x_2 &= \\dot{x}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "å› æ­¤ï¼š\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\dot{x}_1 &= x_2 \\\\\n",
    "\\dot{x}_2 &= \\ddot{x} = 6x_2 - 9x_1 + u\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### âœ… å¾—åˆ°çš„ä¸€é˜¶ç³»ç»Ÿï¼ˆçŠ¶æ€ç©ºé—´è¡¨ç¤ºï¼‰\n",
    "\n",
    "æœ€ç»ˆå¾—åˆ°å¦‚ä¸‹çš„çŠ¶æ€ç©ºé—´å½¢å¼ï¼š\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\dot{x}_1 &= x_2 \\\\\n",
    "\\dot{x}_2 &= 6x_2 - 9x_1 + u\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### ğŸ” æ€»ç»“æ“ä½œæ­¥éª¤\n",
    "\n",
    "1. **æŠŠæœ€é«˜é˜¶å¯¼æ•°ç§»åˆ°ç­‰å¼å·¦è¾¹**\n",
    "2. **å¼•å…¥æ–°å˜é‡è¡¨ç¤ºåŸå§‹å˜é‡åŠå…¶å¯¼æ•°**\n",
    "3. **å†™å‡ºæ¯ä¸ªæ–°å˜é‡çš„ä¸€é˜¶å¯¼æ•°è¡¨è¾¾å¼**\n",
    "4. **å¾—åˆ°ä¸€ç»„ä¸€é˜¶å¾®åˆ†æ–¹ç¨‹**\n",
    "\n",
    "è¿™ç§æ–¹æ³•åœ¨æ§åˆ¶ç†è®ºã€ç³»ç»Ÿå»ºæ¨¡ã€æ•°å€¼æ±‚è§£ç­‰åœºæ™¯ä¸­éå¸¸å¸¸è§å’Œæœ‰ç”¨ã€‚\n",
    "\n",
    "\n",
    "### First Order Differential Equations In State-Space Form (æŠŠä»»ä½•high order to first order linear matrix)\n",
    "\n",
    "Substituting the newly defined variables from the previous section:\n",
    "\n",
    "$$\\frac{dx_1}{dt} = x_2,\\,  \n",
    "\\frac{dx_2}{dt} = x_3, \\, ..., \\, \n",
    "\\frac{dx_{n-1}}{dt} = x_n$$\n",
    "\n",
    "into the first order equations yields: \n",
    "\n",
    "$$\\frac{dx_n}{dt} = \\frac{1}{a_n}\\sum\\limits_{i=0}^{n-1}a_ix_{i+1} + \\frac{1}{a_n}u\n",
    "$$\n",
    "\n",
    "\n",
    "Using vector-matrix notation we have:\n",
    "\n",
    "$$\\begin{bmatrix}\\frac{dx_1}{dt} \\\\ \\frac{dx_2}{dt} \\\\ \\vdots \\\\ \\frac{dx_n}{dt}\\end{bmatrix} = \n",
    "\\begin{bmatrix}\\dot x_1 \\\\ \\dot x_2 \\\\ \\vdots \\\\ \\dot x_n\\end{bmatrix}=\n",
    "\\begin{bmatrix}0 & 1 & 0 &\\cdots & 0 \\\\\n",
    "0 & 0 & 1 & \\cdots & 0 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "-\\frac{a_0}{a_n} & -\\frac{a_1}{a_n} & -\\frac{a_2}{a_n} & \\cdots & -\\frac{a_{n-1}}{a_n}\\end{bmatrix}\n",
    "\\begin{bmatrix}x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n\\end{bmatrix} + \n",
    "\\begin{bmatrix}0 \\\\ 0 \\\\ \\vdots \\\\ \\frac{1}{a_n}\\end{bmatrix}u$$\n",
    "\n",
    "which we then write as $\\dot{\\mathbf x} = \\mathbf{Ax} + \\mathbf{B}u$.\n",
    "\n",
    "### Finding the Fundamental Matrix for Time Invariant Systems\n",
    "\n",
    "We express the system equations in state-space form with\n",
    "\n",
    "$$ \\dot{\\mathbf x} = \\mathbf{Ax}$$\n",
    "\n",
    "where $\\mathbf A$ is the system dynamics matrix, and want to find the *fundamental matrix* $\\mathbf F$ that propagates the state $\\mathbf x$ over the interval $\\Delta t$ with the equation\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbf x(t_k) = \\mathbf F(\\Delta t)\\mathbf x(t_{k-1})\\end{aligned}$$\n",
    "\n",
    "In other words, $\\mathbf A$ is a set of continuous differential equations, and we need $\\mathbf F$ to be a set of discrete linear equations that computes the change in $\\mathbf A$ over a discrete time step.\n",
    "\n",
    "It is conventional to drop the $t_k$ and $(\\Delta t)$ and use the notation\n",
    "\n",
    "$$\\mathbf x_k = \\mathbf {Fx}_{k-1}$$\n",
    "\n",
    "Broadly speaking there are three common ways to find this matrix for Kalman filters. The technique most often used is the matrix exponential. Linear Time Invariant Theory, also known as LTI System Theory, is a second technique. Finally, there are numerical techniques. You may know of others, but these three are what you will most likely encounter in the Kalman filter literature and praxis.\n",
    "\n",
    "### ğŸ§  å¦‚ä½•ç†è§£â€œTime Invariant System çš„ Fundamental Matrixï¼ˆåŸºç¡€çŸ©é˜µï¼‰â€\n",
    "\n",
    "æˆ‘ä»¬ä»çŠ¶æ€ç©ºé—´æ–¹ç¨‹å‡ºå‘ï¼š\n",
    "\n",
    "$$\n",
    "\\dot{\\mathbf x} = \\mathbf{A}\\mathbf{x}\n",
    "$$\n",
    "\n",
    "å…¶ä¸­ $\\mathbf{A}$ æ˜¯ç³»ç»Ÿçš„åŠ¨æ€çŸ©é˜µï¼ˆå¸¸æ•°çŸ©é˜µï¼Œè¡¨ç¤ºç³»ç»Ÿçš„è¿ç»­æ—¶é—´è¡Œä¸ºï¼‰ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ‰¾åˆ°ä¸€ä¸ª *fundamental matrix*ï¼ˆåŸºç¡€çŸ©é˜µï¼‰$\\mathbf{F}$ï¼Œå®ƒå¯ä»¥å°†çŠ¶æ€åœ¨ç¦»æ•£æ—¶é—´æ­¥ä¹‹é—´ä¼ æ’­ï¼Œå³ï¼š\n",
    "\n",
    "$$\n",
    "\\mathbf{x}(t_k) = \\mathbf{F}(\\Delta t)\\mathbf{x}(t_{k-1})\n",
    "$$\n",
    "\n",
    "è¿™æ„å‘³ç€ï¼šè™½ç„¶ $\\mathbf{A}$ æ˜¯ä¸€ä¸ªæè¿°è¿ç»­å˜åŒ–çš„å¾®åˆ†æ–¹ç¨‹ç³»ç»Ÿï¼Œæˆ‘ä»¬éœ€è¦å°†å®ƒâ€œç¦»æ•£åŒ–â€ï¼Œå¾—åˆ°ä¸€ä¸ªèƒ½ç”¨æ¥åœ¨æœ‰é™æ—¶é—´æ­¥å†…ä¼ æ’­çŠ¶æ€çš„çº¿æ€§å˜æ¢ $\\mathbf{F}$ã€‚\n",
    "\n",
    "ä¸ºäº†ç®€åŒ–è®°å·ï¼Œæˆ‘ä»¬é€šå¸¸çœç•¥æ—¶é—´å˜é‡ï¼Œå†™æˆæ›´ç®€æ´çš„å½¢å¼ï¼š\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_k = \\mathbf{F}\\mathbf{x}_{k-1}\n",
    "$$\n",
    "\n",
    "### âœ… ä¸ºä»€ä¹ˆéœ€è¦ $\\mathbf{F}$ï¼Ÿ\n",
    "\n",
    "å› ä¸ºåœ¨è®¸å¤šå®é™…åº”ç”¨ï¼ˆæ¯”å¦‚ Kalman Filterï¼‰ä¸­ï¼Œæˆ‘ä»¬æ˜¯åœ¨ **ç¦»æ•£çš„æ—¶é—´æ­¥é•¿ $\\Delta t$** ä¸‹å¯¹ç³»ç»Ÿå»ºæ¨¡çš„ã€‚ä½†ç³»ç»Ÿçš„çœŸå®åŠ¨åŠ›å­¦æ˜¯è¿ç»­çš„ï¼Œå› æ­¤éœ€è¦ä» $\\mathbf{A}$ æ¨å¯¼å‡ºä¸€ä¸ªç¦»æ•£çš„å˜æ¢ $\\mathbf{F}$ã€‚\n",
    "\n",
    "### âœ… å¦‚ä½•æ±‚ $\\mathbf{F}$ï¼Ÿ\n",
    "\n",
    "åœ¨å¡å°”æ›¼æ»¤æ³¢ï¼ˆKalman Filterï¼‰ç›¸å…³æ–‡çŒ®ä¸­ï¼Œå¸¸è§çš„ä¸‰ç§æ–¹æ³•æœ‰ï¼š\n",
    "\n",
    "1. **çŸ©é˜µæŒ‡æ•°æ³•ï¼ˆMatrix Exponentialï¼‰**ï¼šæœ€å¸¸ç”¨çš„æ–¹æ³•  \n",
    "   $$\n",
    "   \\mathbf{F} = e^{\\mathbf{A} \\Delta t}\n",
    "   $$\n",
    "   é€šå¸¸é€šè¿‡æ³°å‹’å±•å¼€æˆ–æ•°å€¼æ–¹æ³•è®¡ç®—ã€‚\n",
    "\n",
    "2. **çº¿æ€§æ—¶ä¸å˜ç³»ç»Ÿç†è®ºï¼ˆLTI System Theoryï¼‰**ï¼šé€‚ç”¨äºå·²çŸ¥ $\\mathbf{A}$ ç‰¹å¾ç»“æ„çš„ç³»ç»Ÿï¼ŒåŸºäºè§£æè§£è¿›è¡Œæ¨å¯¼ã€‚\n",
    "\n",
    "3. **æ•°å€¼æ–¹æ³•ï¼ˆNumerical Techniquesï¼‰**ï¼šå¦‚æ•°å€¼ç§¯åˆ†ã€æ¬§æ‹‰æ³•ã€RKæ–¹æ³•ï¼Œé€‚ç”¨äºå¤æ‚æˆ–éçº¿æ€§ç³»ç»Ÿçš„è¿‘ä¼¼è®¡ç®—ã€‚\n",
    "\n",
    "### ğŸ“Œ å°ç»“\n",
    "\n",
    "- $\\dot{\\mathbf{x}} = \\mathbf{A}\\mathbf{x}$ æ˜¯è¿ç»­ç³»ç»Ÿçš„æè¿°  \n",
    "- $\\mathbf{x}_k = \\mathbf{F}\\mathbf{x}_{k-1}$ æ˜¯ç¦»æ•£ç³»ç»Ÿçš„æè¿°  \n",
    "- å…³é”®ä»»åŠ¡æ˜¯ï¼š**ä» $\\mathbf{A}$ æ¨å¯¼å‡º $\\mathbf{F}$ï¼Œç”¨äºç¦»æ•£æ—¶é—´åˆ†æ**\n",
    "\n",
    "è¿™äº›æŠ€å·§åœ¨å¡å°”æ›¼æ»¤æ³¢å™¨ã€æœºå™¨äººã€æ§åˆ¶ç³»ç»Ÿå»ºæ¨¡ä¸­éƒ½éå¸¸é‡è¦ã€‚\n",
    "\n",
    "### The Matrix Exponential\n",
    "\n",
    "The solution to the equation $\\frac{dx}{dt} = kx$ can be found by:\n",
    "\n",
    "$$\\begin{gathered}\\frac{dx}{dt} = kx \\\\\n",
    "\\frac{dx}{x} = k\\, dt \\\\\n",
    "\\int \\frac{1}{x}\\, dx = \\int k\\, dt \\\\\n",
    "\\log x = kt + c \\\\\n",
    "x = e^{kt+c} \\\\\n",
    "x = e^ce^{kt} \\\\\n",
    "x = c_0e^{kt}\\end{gathered}$$\n",
    "\n",
    "When $t=0$, $x=x_0$. Substitute these to equation above.\n",
    "\n",
    "$$\\begin{gathered}x_0 = c_0e^{k(0)} \\\\\n",
    "x_0 = c_01 \\\\\n",
    "x_0 = c_0 \\\\\n",
    "x = x_0e^{kt}\\end{gathered}$$\n",
    "\n",
    "Using similar math, the solution to the first-order equation \n",
    "\n",
    "$$\\dot{\\mathbf x} = \\mathbf{Ax} ,\\, \\, \\, \\mathbf x(0) = \\mathbf x_0$$\n",
    "\n",
    "where $\\mathbf A$ is a constant matrix, is\n",
    "\n",
    "$$\\mathbf x = e^{\\mathbf At}\\mathbf x_0$$\n",
    "\n",
    "Substituting $F = e^{\\mathbf At}$, we can write \n",
    "\n",
    "$$\\mathbf x_k = \\mathbf F\\mathbf x_{k-1}$$\n",
    "\n",
    "which is the form we are looking for! We have reduced the problem of finding the fundamental matrix to one of finding the value for $e^{\\mathbf At}$.\n",
    "\n",
    "$e^{\\mathbf At}$ is known as the [matrix exponential](https://en.wikipedia.org/wiki/Matrix_exponential). It can be computed with this power series:\n",
    "\n",
    "$$e^{\\mathbf At} = \\mathbf{I} + \\mathbf{A}t  + \\frac{(\\mathbf{A}t)^2}{2!} + \\frac{(\\mathbf{A}t)^3}{3!} + ... $$\n",
    "\n",
    "That series is found by doing a Taylor series expansion of $e^{\\mathbf At}$, which I will not cover here.\n",
    "\n",
    "Let's use this to find the solution to Newton's equations. Using $v$ as a substitution for $\\dot x$, and assuming constant velocity we get the linear matrix-vector form \n",
    "\n",
    "$$\\begin{bmatrix}\\dot x \\\\ \\dot v\\end{bmatrix} =\\begin{bmatrix}0&1\\\\0&0\\end{bmatrix} \\begin{bmatrix}x \\\\ v\\end{bmatrix}$$\n",
    "\n",
    "This is a first order differential equation, so we can set $\\mathbf{A}=\\begin{bmatrix}0&1\\\\0&0\\end{bmatrix}$ and solve the following equation. I have substituted the interval $\\Delta t$ for $t$ to emphasize that the fundamental matrix is discrete:\n",
    "\n",
    "$$\\mathbf F = e^{\\mathbf A\\Delta t} = \\mathbf{I} + \\mathbf A\\Delta t  + \\frac{(\\mathbf A\\Delta t)^2}{2!} + \\frac{(\\mathbf A\\Delta t)^3}{3!} + ... $$\n",
    "\n",
    "If you perform the multiplication you will find that $\\mathbf{A}^2=\\begin{bmatrix}0&0\\\\0&0\\end{bmatrix}$, which means that all higher powers of $\\mathbf{A}$ are also $\\mathbf{0}$. Thus we get an exact answer without an infinite number of terms:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf F &=\\mathbf{I} + \\mathbf A \\Delta t + \\mathbf{0} \\\\\n",
    "&= \\begin{bmatrix}1&0\\\\0&1\\end{bmatrix} + \\begin{bmatrix}0&1\\\\0&0\\end{bmatrix}\\Delta t\\\\\n",
    "&= \\begin{bmatrix}1&\\Delta t\\\\0&1\\end{bmatrix}\n",
    "\\end{aligned}$$\n",
    "\n",
    "We plug this into $\\mathbf x_k= \\mathbf{Fx}_{k-1}$ to get\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x_k &=\\begin{bmatrix}1&\\Delta t\\\\0&1\\end{bmatrix}x_{k-1}\n",
    "\\end{aligned}$$\n",
    "\n",
    "You will recognize this as the matrix we derived analytically for the constant velocity Kalman filter in the **Multivariate Kalman Filter** chapter.\n",
    "\n",
    "SciPy's linalg module includes a routine `expm()` to compute the matrix exponential. It does not use the Taylor series method, but the [PadÃ© Approximation](https://en.wikipedia.org/wiki/Pad%C3%A9_approximant). There are many (at least 19) methods to compute the matrix exponential, and all suffer from numerical difficulties[1]. You should be aware of the problems, especially when $\\mathbf A$ is large. If you search for \"pade approximation matrix exponential\" you will find many publications devoted to this problem. \n",
    "\n",
    "In practice this may not be of concern to you as for the Kalman filter we normally just take the first two terms of the Taylor series. But don't assume my treatment of the problem is complete and run off and try to use this technique for other problem without doing a numerical analysis of the performance of this technique. Interestingly, one of the favored ways of solving $e^{\\mathbf At}$ is to use a generalized ode solver. In other words, they do the opposite of what we do - turn $\\mathbf A$ into a set of differential equations, and then solve that set using numerical techniques! \n",
    "\n",
    "Here is an example of using `expm()` to solve $e^{\\mathbf At}$.\n",
    "\n",
    "### ğŸ§  ä¸ºä»€ä¹ˆçŸ©é˜µæŒ‡æ•° $e^{\\mathbf{A}t}$ åªèƒ½ç”¨äº LTI ç³»ç»Ÿï¼Ÿ\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… é¦–å…ˆï¼Œä»€ä¹ˆæ˜¯çŸ©é˜µæŒ‡æ•°ï¼Ÿ\n",
    "\n",
    "å¯¹äºä¸€ä¸ª LTI ç³»ç»Ÿï¼š\n",
    "\n",
    "$$\n",
    "\\dot{\\mathbf{x}}(t) = \\mathbf{A} \\mathbf{x}(t)\n",
    "$$\n",
    "\n",
    "æˆ‘ä»¬å¯ä»¥å†™å‡ºå®ƒçš„è§£ä¸ºï¼š\n",
    "\n",
    "$$\n",
    "\\mathbf{x}(t) = e^{\\mathbf{A}t} \\mathbf{x}(0)\n",
    "$$\n",
    "\n",
    "å…¶ä¸­çŸ©é˜µæŒ‡æ•°çš„å®šä¹‰ä¸ºï¼š\n",
    "\n",
    "$$\n",
    "e^{\\mathbf{A}t} = \\sum_{k=0}^{\\infty} \\frac{(\\mathbf{A}t)^k}{k!}\n",
    "$$\n",
    "\n",
    "å®ƒæ˜¯ä¸€ä¸ª**å°é—­å½¢å¼çš„è§£æè§£**ï¼Œç”¨äºæè¿°ç³»ç»ŸçŠ¶æ€éšæ—¶é—´æ¼”åŒ–çš„è¿‡ç¨‹ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ›‘ ä¸ºä»€ä¹ˆè¿™ä¸ªæ–¹æ³•**ä¸èƒ½ç”¨äº LTV ç³»ç»Ÿ**ï¼Ÿ\n",
    "\n",
    "å¦‚æœæˆ‘ä»¬ç°åœ¨æœ‰ä¸€ä¸ª **çº¿æ€§æ—¶å˜ç³»ç»Ÿï¼ˆLTVï¼‰**ï¼š\n",
    "\n",
    "$$\n",
    "\\dot{\\mathbf{x}}(t) = \\mathbf{A}(t) \\mathbf{x}(t)\n",
    "$$\n",
    "\n",
    "ä½ ä¼šå‘ç°è¿™ä¸ªæ–¹ç¨‹æ›´å¤æ‚ï¼Œå› ä¸ºï¼š\n",
    "\n",
    "- ç³»ç»ŸçŸ©é˜µ $\\mathbf{A}(t)$ æ˜¯**éšæ—¶é—´å˜åŒ–çš„å‡½æ•°**\n",
    "- é‚£ä¹ˆåœ¨ä¸åŒæ—¶é—´ï¼Œç³»ç»Ÿçš„è¡Œä¸ºå®Œå…¨ä¸åŒï¼ˆä¸æ˜¯å¸¸æ•°çŸ©é˜µï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "### âŒ ä¸ºä»€ä¹ˆä¸èƒ½ç”¨ $e^{\\mathbf{A}(t)t}$ï¼Ÿ\n",
    "\n",
    "ä½ å¯èƒ½ä¼šé—®ï¼šé‚£æˆ‘ä»¬å†™æˆï¼š\n",
    "\n",
    "$$\n",
    "\\mathbf{x}(t) = e^{\\mathbf{A}(t)t} \\mathbf{x}(0)\n",
    "$$\n",
    "\n",
    "è¡Œä¸è¡Œï¼Ÿ\n",
    "\n",
    "ç­”æ¡ˆæ˜¯ **ä¸è¡Œï¼**\n",
    "\n",
    "å› ä¸ºçŸ©é˜µæŒ‡æ•°çš„å®šä¹‰æ˜¯åŸºäºå¸¸æ•°çŸ©é˜µçš„ï¼š\n",
    "\n",
    "$$\n",
    "\\frac{d}{dt} \\left( e^{\\mathbf{A}t} \\right) = \\mathbf{A} e^{\\mathbf{A}t}\n",
    "$$\n",
    "\n",
    "ä½†å¦‚æœ $\\mathbf{A}$ æ˜¯ä¸€ä¸ªå‡½æ•° $\\mathbf{A}(t)$ï¼Œé‚£å¯¼æ•°å°±ä¸æ˜¯ç®€å•çš„ï¼š\n",
    "\n",
    "$$\n",
    "\\frac{d}{dt} \\left( e^{\\mathbf{A}(t)t} \\right) \\neq \\mathbf{A}(t) e^{\\mathbf{A}(t)t}\n",
    "$$\n",
    "\n",
    "å®é™…ä¸Šï¼Œå®ƒåŒ…å«ä¸€ä¸ªéå¸¸å¤æ‚çš„æ±‚å¯¼é“¾å¼è§„åˆ™ï¼Œç”šè‡³å¯èƒ½**æ— æ³•ç”¨å°é—­å½¢å¼è¡¨è¾¾**ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”§ é‚£ä¹ˆ LTV ç³»ç»Ÿæ€ä¹ˆæ±‚è§£ï¼Ÿ\n",
    "\n",
    "LTV ç³»ç»Ÿçš„ä¸€èˆ¬è§£æ˜¯ç”¨æ‰€è°“çš„ **çŠ¶æ€è½¬ç§»çŸ©é˜µ $\\Phi(t, t_0)$**ï¼š\n",
    "\n",
    "$$\n",
    "\\mathbf{x}(t) = \\Phi(t, t_0) \\mathbf{x}(t_0)\n",
    "$$\n",
    "\n",
    "è¿™ä¸ª $\\Phi$ **ä¸ç­‰äº** ç®€å•çš„ $e^{\\mathbf{A}(t)(t - t_0)}$ï¼Œè€Œæ˜¯é€šè¿‡**æ—¶é—´æœ‰åºç§¯åˆ†**å¾—åˆ°çš„ï¼š\n",
    "\n",
    "$$\n",
    "\\Phi(t, t_0) = \\mathcal{T} \\exp\\left( \\int_{t_0}^t \\mathbf{A}(\\tau) d\\tau \\right)\n",
    "$$\n",
    "\n",
    "å…¶ä¸­ $\\mathcal{T}$ æ˜¯**æ—¶é—´æœ‰åºç®—ç¬¦**ï¼Œè¡¨ç¤ºä¸åŒæ—¶é—´ç‚¹çš„çŸ©é˜µä¹˜æ³•**é¡ºåº**è¦è€ƒè™‘åœ¨å†…ï¼Œè¿™å°±éå¸¸å¤æ‚ï¼Œé€šå¸¸æ²¡æœ‰è§£æè§£ï¼Œåªèƒ½ç”¨æ•°å€¼æ–¹æ³•æ±‚è§£ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… æ€»ç»“ä¸€å¥è¯ï¼š\n",
    "\n",
    "> çŸ©é˜µæŒ‡æ•° $e^{\\mathbf{A}t}$ åªèƒ½ç”¨äº LTI ç³»ç»Ÿï¼Œæ˜¯å› ä¸ºå®ƒå‡è®¾ $\\mathbf{A}$ æ˜¯å¸¸æ•°çŸ©é˜µã€‚è€Œåœ¨ LTV ç³»ç»Ÿä¸­ï¼Œ$\\mathbf{A}(t)$ éšæ—¶é—´å˜åŒ–ï¼Œå¯¼è‡´çŸ©é˜µæŒ‡æ•°çš„æ€§è´¨ä¸å†æˆç«‹ï¼Œå¿…é¡»ä½¿ç”¨çŠ¶æ€è½¬ç§»çŸ©é˜µ $\\Phi(t, t_0)$ å’Œæ•°å€¼æ–¹æ³•æ¥æ±‚è§£ã€‚\n",
    "\n",
    "\n",
    "### ğŸ§  What is PadÃ© Approximation?\n",
    "\n",
    "PadÃ© Approximationï¼ˆå¸•å¾·è¿‘ä¼¼ï¼‰æ˜¯ä¸€ç§æ¯”æ³°å‹’çº§æ•°æ›´ç²¾ç¡®åœ°è¿‘ä¼¼å‡½æ•°çš„æ–¹æ³•ï¼Œç‰¹åˆ«é€‚ç”¨äºéœ€è¦ç”¨**æœ‰ç†å‡½æ•°ï¼ˆåˆ†å¼ï¼‰**æ¥é€¼è¿‘æŒ‡æ•°å‡½æ•°çš„åœºæ™¯ã€‚åœ¨ç³»ç»Ÿå»ºæ¨¡å’Œæ§åˆ¶ç†è®ºä¸­ï¼Œå®ƒå¸¸ç”¨äº**è¿‘ä¼¼çŸ©é˜µæŒ‡æ•° $e^{\\mathbf{A} \\Delta t}$**ï¼Œä»¥å¾—åˆ°çŠ¶æ€è½¬ç§»çŸ©é˜µ $\\mathbf{F}$ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Œ å®šä¹‰\n",
    "\n",
    "PadÃ© Approximation æ˜¯ä¸€ä¸ªæœ‰ç†å‡½æ•°é€¼è¿‘ï¼Œå³ï¼š\n",
    "\n",
    "$$\n",
    "e^{x} \\approx \\frac{P(x)}{Q(x)} = \\frac{a_0 + a_1 x + \\dots + a_m x^m}{b_0 + b_1 x + \\dots + b_n x^n}\n",
    "$$\n",
    "\n",
    "å…¶ä¸­ï¼š\n",
    "- $P(x)$ æ˜¯åˆ†å­å¤šé¡¹å¼ï¼Œæ¬¡æ•°ä¸º $m$\n",
    "- $Q(x)$ æ˜¯åˆ†æ¯å¤šé¡¹å¼ï¼Œæ¬¡æ•°ä¸º $n$\n",
    "- ç³»æ•°é€šè¿‡åŒ¹é…æ³°å‹’å±•å¼€çš„å‰å‡ é¡¹æ¥æ±‚è§£\n",
    "\n",
    "å¸¸ç”¨çš„è¿‘ä¼¼æ˜¯ $[n/n]$ PadÃ© è¿‘ä¼¼ï¼Œå³åˆ†å­å’Œåˆ†æ¯å¤šé¡¹å¼æ¬¡æ•°ç›¸ç­‰ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ’¡ åœ¨æ§åˆ¶ç³»ç»Ÿä¸­çš„åº”ç”¨ï¼šè¿‘ä¼¼çŸ©é˜µæŒ‡æ•°\n",
    "\n",
    "åœ¨ç¦»æ•£åŒ–è¿ç»­ç³»ç»Ÿæ—¶ï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®—ï¼š\n",
    "\n",
    "$$\n",
    "\\mathbf{F} = e^{\\mathbf{A} \\Delta t}\n",
    "$$\n",
    "\n",
    "ç›´æ¥è®¡ç®—è¿™ä¸ªçŸ©é˜µæŒ‡æ•°å¯èƒ½æ¯”è¾ƒå›°éš¾ã€‚ä½¿ç”¨ PadÃ© Approximation å¯ä»¥æŠŠå®ƒè½¬åŒ–ä¸ºï¼š\n",
    "\n",
    "$$\n",
    "e^{\\mathbf{A} \\Delta t} \\approx \\left(I - \\frac{1}{2} \\mathbf{A} \\Delta t + \\frac{1}{12} \\mathbf{A}^2 \\Delta t^2 - \\dots \\right)^{-1} \\left(I + \\frac{1}{2} \\mathbf{A} \\Delta t + \\frac{1}{12} \\mathbf{A}^2 \\Delta t^2 + \\dots \\right)\n",
    "$$\n",
    "\n",
    "è¿™æ˜¯ä¸€ç§æ¯”ç›´æ¥æˆªæ–­æ³°å‹’å±•å¼€æ›´ç¨³å®šã€æ›´å‡†ç¡®çš„æ–¹æ³•ï¼Œå°¤å…¶æ˜¯åœ¨ $\\mathbf{A} \\Delta t$ æ¯”è¾ƒå¤§çš„æƒ…å†µä¸‹ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… æ€»ç»“\n",
    "\n",
    "- å¸•å¾·è¿‘ä¼¼ç”¨ä¸€ä¸ªæœ‰ç†å‡½æ•°æ¥ä»£æ›¿æŒ‡æ•°å‡½æ•°ï¼Œæ›´å‡†ç¡®æ›´ç¨³å®š\n",
    "- åœ¨ Kalman Filter ä¸­å¸¸ç”¨äºè®¡ç®— $e^{\\mathbf{A} \\Delta t}$\n",
    "- æ˜¯æ•°å€¼è®¡ç®—ä¸­å¤„ç†çŸ©é˜µæŒ‡æ•°çš„ç»å…¸æ–¹æ³•ä¹‹ä¸€\n",
    "\n",
    "éœ€è¦æˆ‘ä¸¾ä¸ªç”¨ NumPy å®ç°çŸ©é˜µ PadÃ© Approximation çš„ä¾‹å­å—ï¼Ÿ\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy.linalg import expm\n",
    "\n",
    "dt = 0.1\n",
    "A = np.array([[0, 1], \n",
    "              [0, 0]])\n",
    "expm(A*dt)\n",
    "\n",
    "### Time Invariance\n",
    "\n",
    "If the behavior of the system depends on time we can say that a dynamic system is described by the first-order differential equation\n",
    "\n",
    "$$ g(t) = \\dot x$$\n",
    "\n",
    "However, if the system is *time invariant* the equation is of the form:\n",
    "\n",
    "$$ f(x) = \\dot x$$\n",
    "\n",
    "What does *time invariant* mean? Consider a home stereo. If you input a signal $x$ into it at time $t$, it will output some signal $f(x)$. If you instead perform the input at time $t + \\Delta t$ the output signal will be the same $f(x)$, shifted in time.\n",
    "\n",
    "A counter-example is $x(t) = \\sin(t)$, with the system  $f(x) = t\\,  x(t) = t \\sin(t)$. This is not time invariant; the value will be different at different times due to the multiplication by t.  An aircraft is not time invariant. If you make a control input to the aircraft at a later time its behavior will be different because it will have burned fuel and thus lost weight. Lower weight results in different behavior.\n",
    "\n",
    "We can solve these equations by integrating each side. I demonstrated integrating the time invariant system $v = \\dot x$ above. However, integrating the time invariant equation $\\dot x = f(x)$ is not so straightforward. Using the *separation of variables* techniques we divide by $f(x)$ and move the $dt$ term to the right so we can integrate each side:\n",
    "\n",
    "$$\\begin{gathered}\n",
    "\\frac{dx}{dt} = f(x) \\\\\n",
    "\\int^x_{x_0} \\frac{1}{f(x)} dx = \\int^t_{t_0} dt\n",
    "\\end{gathered}$$\n",
    "\n",
    "If we let $F(x) = \\int \\frac{1}{f(x)} dx$ we get\n",
    "\n",
    "$$F(x) - F(x_0) = t-t_0$$\n",
    "\n",
    "We then solve for x with\n",
    "\n",
    "$$\\begin{gathered}\n",
    "F(x) = t - t_0 + F(x_0) \\\\\n",
    "x = F^{-1}[t-t_0 + F(x_0)]\n",
    "\\end{gathered}$$\n",
    "\n",
    "In other words, we need to find the inverse of $F$. This is not trivial, and a significant amount of coursework in a STEM education is devoted to finding tricky, analytic solutions to this problem. \n",
    "\n",
    "However, they are tricks, and many simple forms of $f(x)$ either have no closed form solution or pose extreme difficulties. Instead, the practicing engineer turns to state-space methods to find approximate solutions.\n",
    "\n",
    "The advantage of the matrix exponential is that we can use it for any arbitrary set of differential equations which are *time invariant*. However, we often use this technique even when the equations are not time invariant. As an aircraft flies it burns fuel and loses weight. However, the weight loss over one second is negligible, and so the system is nearly linear over that time step. Our answers will still be reasonably accurate so long as the time step is short.\n",
    "\n",
    "### ğŸ•’ Time Invarianceï¼ˆæ—¶ä¸å˜æ€§ï¼‰\n",
    "\n",
    "åŠ¨æ€ç³»ç»Ÿå¦‚æœå…¶è¡Œä¸ºéšæ—¶é—´è€Œå˜åŒ–ï¼Œæˆ‘ä»¬å¯ä»¥å°†å…¶æè¿°ä¸ºå¦‚ä¸‹å½¢å¼çš„ä¸€é˜¶å¾®åˆ†æ–¹ç¨‹ï¼š\n",
    "\n",
    "$$\n",
    "\\dot{x} = g(t)\n",
    "$$\n",
    "\n",
    "ä½†å¦‚æœç³»ç»Ÿæ˜¯ **time invariantï¼ˆæ—¶ä¸å˜çš„ï¼‰**ï¼Œåˆ™å¯ä»¥è¡¨ç¤ºä¸ºï¼š\n",
    "\n",
    "$$\n",
    "\\dot{x} = f(x)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… ä»€ä¹ˆæ˜¯ Time Invarianceï¼Ÿ\n",
    "\n",
    "æ—¶ä¸å˜æ€§æ„å‘³ç€ï¼š**ç³»ç»Ÿå¯¹è¾“å…¥çš„å“åº”ä¸ä¾èµ–äºå‘ç”Ÿçš„æ—¶é—´ï¼Œåªä¾èµ–äºè¾“å…¥æœ¬èº«ã€‚**\n",
    "\n",
    "ä¸¾ä¸ªä¾‹å­ï¼šè€ƒè™‘ä¸€ä¸ªå®¶åº­éŸ³å“ã€‚å¦‚æœä½ åœ¨æ—¶é—´ $t$ ç»™å®ƒä¸€ä¸ªä¿¡å· $x$ï¼Œå®ƒè¾“å‡ºä¿¡å· $f(x)$ã€‚å¦‚æœä½ æ”¹ä¸ºåœ¨ $t + \\Delta t$ è¾“å…¥åŒæ ·çš„ä¿¡å· $x$ï¼Œé‚£ä¹ˆå®ƒä¾æ—§ä¼šè¾“å‡º $f(x)$ï¼Œåªæ˜¯**æ•´ä½“æ—¶åºå‘åå¹³ç§»äº† $\\Delta t$**ï¼Œè¾“å‡ºæœ¬èº«ä¸å˜ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### âŒ éæ—¶ä¸å˜ç³»ç»Ÿçš„åä¾‹\n",
    "\n",
    "å‡è®¾è¾“å…¥ä¸º $x(t) = \\sin(t)$ï¼Œç³»ç»Ÿä¸ºï¼š\n",
    "\n",
    "$$\n",
    "f(x) = t \\cdot x(t) = t \\cdot \\sin(t)\n",
    "$$\n",
    "\n",
    "è¿™ä¸ªç³»ç»Ÿä¸æ˜¯æ—¶ä¸å˜çš„ï¼Œå› ä¸ºè¾“å‡ºä¾èµ–äºæ—¶é—´ $t$ï¼Œä¸åŒæ—¶é—´ç‚¹è¾“å…¥åŒæ ·çš„ä¿¡å·ä¼šå¾—åˆ°ä¸åŒè¾“å‡ºã€‚\n",
    "\n",
    "å¦ä¸€ä¸ªåä¾‹æ˜¯é£æœºã€‚ä½ åœ¨èµ·é£æ—¶å¯¹å…¶æ§åˆ¶å’Œåœ¨é£è¡Œ 1 å°æ—¶åå¯¹å…¶æ§åˆ¶ï¼Œååº”ä¼šä¸åŒï¼Œå› ä¸ºé£æœºå˜è½»äº†ï¼ˆç‡ƒæ²¹æ¶ˆè€—ï¼‰ã€‚è¿™å°±ä¸æ˜¯ä¸€ä¸ªæ—¶ä¸å˜ç³»ç»Ÿã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§® å¦‚ä½•æ±‚è§£æ—¶ä¸å˜å¾®åˆ†æ–¹ç¨‹ï¼Ÿ\n",
    "\n",
    "è€ƒè™‘ä¸€ä¸ªæ—¶ä¸å˜ç³»ç»Ÿï¼š\n",
    "\n",
    "$$\n",
    "\\dot{x} = f(x)\n",
    "$$\n",
    "\n",
    "è¿™ä¸æ˜¯ä¸€ä¸ªç®€å•çº¿æ€§å½¢å¼çš„æ–¹ç¨‹ï¼Œä¸èƒ½ç›´æ¥ç§¯åˆ†ä¸¤è¾¹æ±‚è§£ã€‚ä½†æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ **å˜é‡åˆ†ç¦»æ³•ï¼ˆseparation of variablesï¼‰**ï¼š\n",
    "\n",
    "$$\n",
    "\\begin{gathered}\n",
    "\\frac{dx}{dt} = f(x) \\\\\n",
    "\\int_{x_0}^{x} \\frac{1}{f(x)} dx = \\int_{t_0}^{t} dt\n",
    "\\end{gathered}\n",
    "$$\n",
    "\n",
    "ä»¤ï¼š\n",
    "\n",
    "$$\n",
    "F(x) = \\int \\frac{1}{f(x)} dx\n",
    "$$\n",
    "\n",
    "é‚£ä¹ˆç»“æœæ˜¯ï¼š\n",
    "\n",
    "$$\n",
    "F(x) - F(x_0) = t - t_0\n",
    "$$\n",
    "\n",
    "æœ€ç»ˆè§£ä¸ºï¼š\n",
    "\n",
    "$$\n",
    "x = F^{-1}\\left[ t - t_0 + F(x_0) \\right]\n",
    "$$\n",
    "\n",
    "ä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬éœ€è¦æ±‚å‡º $F$ çš„åå‡½æ•°ã€‚ä½†è¿™åœ¨æ•°å­¦ä¸Šé€šå¸¸**ä¸å®¹æ˜“**ï¼Œå› ä¸ºå¾ˆå¤š $f(x)$ è¦ä¹ˆæ²¡æœ‰å°é—­è§£ï¼ˆclosed-form solutionï¼‰ï¼Œè¦ä¹ˆéš¾ä»¥æ±‚é€†ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¤– å·¥ç¨‹ä¸­çš„å¤„ç†æ–¹å¼\n",
    "\n",
    "åœ¨å®é™…å·¥ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä¸ä¼šå»è´¹åŠ²åœ°æ±‚è¿™äº›è§£æè§£ï¼Œè€Œæ˜¯ç”¨**çŠ¶æ€ç©ºé—´æ–¹æ³•**æ¥è·å¾—è¿‘ä¼¼è§£ã€‚è¿™ç§æ–¹æ³•æ›´é€šç”¨ã€å¯æ‰©å±•ï¼Œå¹¶ä¸”é€‚ç”¨äºä»»æ„ç»´åº¦çš„å¾®åˆ†æ–¹ç¨‹ç»„ã€‚\n",
    "\n",
    "ç‰¹åˆ«åœ°ï¼Œæˆ‘ä»¬ä¼šç”¨ **çŸ©é˜µæŒ‡æ•°ï¼ˆmatrix exponentialï¼‰** æ¥è§£å†³ **çº¿æ€§æ—¶ä¸å˜ç³»ç»Ÿï¼ˆLTIï¼‰**ï¼š\n",
    "\n",
    "$$\n",
    "\\mathbf{x}(t) = e^{\\mathbf{A} \\Delta t} \\mathbf{x}(t_0)\n",
    "$$\n",
    "\n",
    "å³ä½¿åœ¨æŸäº›éæ—¶ä¸å˜ç³»ç»Ÿä¸­ï¼ˆæ¯”å¦‚é£æœºåœ¨é£è¡Œè¿‡ç¨‹ä¸­è´¨é‡å˜åŒ–ï¼‰ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥è®¤ä¸ºåœ¨**å¾ˆçŸ­çš„æ—¶é—´æ­¥å†…**ç³»ç»Ÿè¿‘ä¼¼æ—¶ä¸å˜ã€‚è¿™æ ·ä¾æ—§å¯ä»¥ä½¿ç”¨çŸ©é˜µæŒ‡æ•°æ–¹æ³•å¾—åˆ°è¶³å¤Ÿç²¾ç¡®çš„è¿‘ä¼¼ç»“æœã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… å°ç»“\n",
    "\n",
    "- æ—¶ä¸å˜ç³»ç»Ÿçš„è¡Œä¸ºåªä¾èµ–äºè¾“å…¥æœ¬èº«ï¼Œè€Œéè¾“å…¥å‘ç”Ÿçš„æ—¶é—´\n",
    "- å¯é€šè¿‡å˜é‡åˆ†ç¦»æ³•ç§¯åˆ†æ±‚è§£ï¼Œä½†éš¾ä»¥å¾—åˆ°è§£æè§£\n",
    "- å®é™…å·¥ç¨‹ä¸­å¸¸ç”¨çŠ¶æ€ç©ºé—´æ–¹æ³•å’ŒçŸ©é˜µæŒ‡æ•°è¿›è¡Œè¿‘ä¼¼è®¡ç®—\n",
    "- å³ä½¿éæ—¶ä¸å˜ç³»ç»Ÿï¼Œåªè¦æ—¶é—´æ­¥è¶³å¤Ÿå°ï¼Œä¹Ÿå¯ä»¥è¿‘ä¼¼ä½¿ç”¨çº¿æ€§æ–¹æ³•å¤„ç†\n",
    "\n",
    "\n",
    "#### Example: Mass-Spring-Damper Model\n",
    "\n",
    "Suppose we wanted to track the motion of a weight on a spring and connected to a damper, such as an automobile's suspension. The equation for the motion with $m$ being the mass, $k$ the spring constant, and $c$ the damping force, under some input $u$ is \n",
    "\n",
    "$$m\\frac{d^2x}{dt^2} + c\\frac{dx}{dt} +kx = u$$\n",
    "\n",
    "For notational convenience I will write that as\n",
    "\n",
    "$$m\\ddot x + c\\dot x + kx = u$$\n",
    "\n",
    "I can turn this into a system of first order equations by setting $x_1(t)=x(t)$, and then substituting as follows:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "x_1 &= x \\\\\n",
    "x_2 &= \\dot x_1 \\\\\n",
    "\\dot x_2 &= \\ddot x_1 = \\ddot x\n",
    "\\end{aligned}$$\n",
    "\n",
    "As is common I dropped the $(t)$ for notational convenience. This gives the equation\n",
    "\n",
    "$$m\\dot x_2 + c x_2 +kx_1 = u$$\n",
    "\n",
    "Solving for $\\dot x_2$ we get a first order equation:\n",
    "\n",
    "$$\\dot x_2 = -\\frac{c}{m}x_2 - \\frac{k}{m}x_1 + \\frac{1}{m}u$$\n",
    "\n",
    "We put this into matrix form:\n",
    "\n",
    "$$\\begin{bmatrix} \\dot x_1 \\\\ \\dot x_2 \\end{bmatrix} = \n",
    "\\begin{bmatrix}0 & 1 \\\\ -k/m & -c/m \\end{bmatrix}\n",
    "\\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} + \n",
    "\\begin{bmatrix} 0 \\\\ 1/m \\end{bmatrix}u$$\n",
    "\n",
    "Now we use the matrix exponential to find the state transition matrix:\n",
    "\n",
    "$$\\Phi(t) = e^{\\mathbf At} = \\mathbf{I} + \\mathbf At  + \\frac{(\\mathbf At)^2}{2!} + \\frac{(\\mathbf At)^3}{3!} + ... $$\n",
    "\n",
    "The first two terms give us\n",
    "\n",
    "$$\\mathbf F = \\begin{bmatrix}1 & t \\\\ -(k/m) t & 1-(c/m) t \\end{bmatrix}$$\n",
    "\n",
    "This may or may not give you enough precision. You can easily check this by computing $\\frac{(\\mathbf At)^2}{2!}$ for your constants and seeing how much this matrix contributes to the results.\n",
    "\n",
    "### Linear Time Invariant Theory\n",
    "\n",
    "[*Linear Time Invariant Theory*](https://en.wikipedia.org/wiki/LTI_system_theory), also known as LTI System Theory, gives us a way to find $\\Phi$ using the inverse Laplace transform. You are either nodding your head now, or completely lost.  I will not be using the Laplace transform in this book. LTI system theory tells us that \n",
    "\n",
    "$$ \\Phi(t) = \\mathcal{L}^{-1}[(s\\mathbf{I} - \\mathbf{A})^{-1}]$$\n",
    "\n",
    "I have no intention of going into this other than to say that the Laplace transform $\\mathcal{L}$ converts a signal into a space $s$ that excludes time, but finding a solution to the equation above is non-trivial. If you are interested, the Wikipedia article on LTI system theory provides an introduction. I mention LTI because you will find some literature using it to design the Kalman filter matrices for difficult problems. \n",
    "\n",
    "### ğŸ§  ä»€ä¹ˆæ˜¯ Linear Time Invariant Theoryï¼Ÿï¼ˆçº¿æ€§æ—¶ä¸å˜ç³»ç»Ÿç†è®ºï¼‰\n",
    "\n",
    "Linear Time Invariantï¼ˆLTIï¼‰Theory æ˜¯ç ”ç©¶ä¸€ç±»ç‰¹æ®Šç³»ç»Ÿçš„æ–¹æ³•ï¼Œè¿™ç±»ç³»ç»Ÿæœ‰ä¸¤ä¸ªå…³é”®ç‰¹æ€§ï¼š\n",
    "\n",
    "- **Linearï¼ˆçº¿æ€§ï¼‰**ï¼šè¾“å…¥å’Œè¾“å‡ºæ»¡è¶³å åŠ æ€§å’Œæ¯”ä¾‹æ€§\n",
    "- **Time Invariantï¼ˆæ—¶ä¸å˜ï¼‰**ï¼šç³»ç»Ÿçš„è¡Œä¸ºä¸ä¼šéšç€æ—¶é—´æ”¹å˜\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ” ä¸¾ä¸ªç®€å•çš„ä¾‹å­ï¼š\n",
    "\n",
    "å‡è®¾ä½ æœ‰ä¸€ä¸ªç³»ç»Ÿï¼Œæ¯”å¦‚ä¸€ä¸ªæ»¤æ³¢å™¨ï¼š\n",
    "\n",
    "- ä½ è¾“å…¥ä¸€ä¸ªä¿¡å· $x(t)$ï¼Œç³»ç»Ÿè¾“å‡º $y(t)$ã€‚\n",
    "- å¦‚æœä½ æŠŠè¾“å…¥ä¿¡å·å»¶è¿Ÿ 5 ç§’ï¼ˆå˜æˆ $x(t - 5)$ï¼‰ï¼Œé‚£ä¹ˆè¾“å‡ºä¹Ÿåªä¼šå»¶è¿Ÿ 5 ç§’ï¼ˆå˜æˆ $y(t - 5)$ï¼‰ã€‚\n",
    "\n",
    "è¿™å°±è¯´æ˜å®ƒæ˜¯ä¸€ä¸ª **æ—¶ä¸å˜ç³»ç»Ÿ**ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "å†çœ‹â€œçº¿æ€§â€ï¼š\n",
    "\n",
    "- å¦‚æœè¾“å…¥æ˜¯ä¸¤ä¸ªä¿¡å· $x_1(t)$ å’Œ $x_2(t)$ï¼Œè¾“å‡ºåˆ†åˆ«æ˜¯ $y_1(t)$ å’Œ $y_2(t)$ï¼Œ\n",
    "- é‚£ä¹ˆè¾“å…¥ $x_1(t) + x_2(t)$ï¼Œè¾“å‡ºå°±æ˜¯ $y_1(t) + y_2(t)$ï¼›\n",
    "- è¾“å…¥ $3x_1(t)$ï¼Œè¾“å‡ºå°±æ˜¯ $3y_1(t)$ã€‚\n",
    "\n",
    "è¿™è¯´æ˜å®ƒæ˜¯ä¸€ä¸ª **çº¿æ€§ç³»ç»Ÿ**ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“ æ•°å­¦æ¨¡å‹å½¢å¼\n",
    "\n",
    "LTI ç³»ç»Ÿå¸¸ç”¨çš„çŠ¶æ€ç©ºé—´å½¢å¼æ˜¯ï¼š\n",
    "\n",
    "$$\n",
    "\\dot{\\mathbf{x}}(t) = \\mathbf{A}\\mathbf{x}(t) + \\mathbf{B}\\mathbf{u}(t) \\\\\n",
    "\\mathbf{y}(t) = \\mathbf{C}\\mathbf{x}(t) + \\mathbf{D}\\mathbf{u}(t)\n",
    "$$\n",
    "\n",
    "å…¶ä¸­ï¼š\n",
    "- $\\mathbf{x}(t)$ æ˜¯ç³»ç»ŸçŠ¶æ€\n",
    "- $\\mathbf{u}(t)$ æ˜¯è¾“å…¥\n",
    "- $\\mathbf{y}(t)$ æ˜¯è¾“å‡º\n",
    "- $\\mathbf{A}, \\mathbf{B}, \\mathbf{C}, \\mathbf{D}$ æ˜¯å¸¸æ•°çŸ©é˜µï¼ˆè¿™ä¹Ÿæ˜¯â€œæ—¶ä¸å˜â€çš„ä½“ç°ï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "### âœ¨ ä¸ºä»€ä¹ˆ LTI å¾ˆé‡è¦ï¼Ÿ\n",
    "\n",
    "- âœ… æ–¹ä¾¿åˆ†æï¼šå¯ä»¥ç”¨çŸ©é˜µæŒ‡æ•° $e^{\\mathbf{A}t}$ æ¥æ±‚è§£\n",
    "- âœ… å¯é¢„æµ‹ï¼šç³»ç»Ÿä¸ä¼šéšæ—¶é—´å˜â€œä¸ªæ€§â€\n",
    "- âœ… æœ‰é—­å¼è§£ï¼šå¯ä»¥ç”¨æ‹‰æ™®æ‹‰æ–¯å˜æ¢ã€å‚…é‡Œå¶å˜æ¢ç­‰æ±‚è§£\n",
    "- âœ… ç”¨é€”å¹¿ï¼šä¿¡å·å¤„ç†ã€æ§åˆ¶ç³»ç»Ÿã€æ»¤æ³¢å™¨ã€ç”µè·¯ã€æŒ¯åŠ¨åˆ†æä¸­éƒ½ç”¨å®ƒ\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸš€ æ€»ç»“ä¸€å¥è¯ï¼š\n",
    "\n",
    "**LTI ç³»ç»Ÿ = ç»“æ„ç®€å• + è¡Œä¸ºç¨³å®š**ï¼Œéå¸¸é€‚åˆå»ºæ¨¡å’Œæ±‚è§£ï¼Œæ˜¯å·¥ç¨‹é‡Œæœ€å¸¸è§ã€æœ€é‡è¦çš„ä¸€ç±»ç³»ç»Ÿæ¨¡å‹ã€‚\n",
    "\n",
    "\n",
    "### ğŸ§  ä»€ä¹ˆæ˜¯ Linear Time Variant Theoryï¼Ÿï¼ˆçº¿æ€§æ—¶å˜ç³»ç»Ÿç†è®ºï¼‰\n",
    "\n",
    "Linear Time Variantï¼ˆLTVï¼‰Theory æ˜¯ç ”ç©¶**çº¿æ€§ä½†éšæ—¶é—´å˜åŒ–çš„ç³»ç»Ÿ**çš„ä¸€å¥—ç†è®ºã€‚å®ƒå’Œ LTIï¼ˆçº¿æ€§æ—¶ä¸å˜ç³»ç»Ÿï¼‰ç›¸æ¯”ï¼Œæœ€å¤§çš„åŒºåˆ«å°±æ˜¯ï¼š**ç³»ç»Ÿçš„è¡Œä¸ºéšæ—¶é—´å˜åŒ–ï¼**\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… ç‰¹å¾å®šä¹‰\n",
    "\n",
    "LTV ç³»ç»Ÿæ»¡è¶³ä¸¤ä¸ªæ¡ä»¶ï¼š\n",
    "\n",
    "- **Linearï¼ˆçº¿æ€§ï¼‰**ï¼šè¾“å…¥å’Œè¾“å‡ºæ»¡è¶³å åŠ æ€§å’Œæ¯”ä¾‹æ€§\n",
    "- **Time Variantï¼ˆæ—¶å˜ï¼‰**ï¼šç³»ç»Ÿçš„å‚æ•°éšæ—¶é—´è€Œå˜\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“ æ•°å­¦è¡¨è¾¾å¼\n",
    "\n",
    "ä¸€ä¸ªå…¸å‹çš„ LTV ç³»ç»Ÿå¯ä»¥å†™æˆçŠ¶æ€ç©ºé—´çš„å½¢å¼å¦‚ä¸‹ï¼š\n",
    "\n",
    "$$\n",
    "\\dot{\\mathbf{x}}(t) = \\mathbf{A}(t)\\mathbf{x}(t) + \\mathbf{B}(t)\\mathbf{u}(t) \\\\\n",
    "\\mathbf{y}(t) = \\mathbf{C}(t)\\mathbf{x}(t) + \\mathbf{D}(t)\\mathbf{u}(t)\n",
    "$$\n",
    "\n",
    "ä½ å¯ä»¥çœ‹åˆ°ï¼Œç³»ç»Ÿçš„çŸ©é˜µ $\\mathbf{A}(t), \\mathbf{B}(t), \\mathbf{C}(t), \\mathbf{D}(t)$ éƒ½æ˜¯ **éšæ—¶é—´å˜åŒ–çš„å‡½æ•°**ï¼Œè€Œä¸åƒ LTI ç³»ç»Ÿä¸­æ˜¯å¸¸æ•°ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ” ä¸¾ä¸ªä¾‹å­\n",
    "\n",
    "å‡è®¾ä¸€è¾†æ±½è½¦åœ¨å¹³è·¯ä¸Šå¼€ï¼Œè´¨é‡æ˜¯ $1000kg$ï¼ŒåŠ é€Ÿåº¦å’Œæ²¹é—¨æˆæ­£æ¯”ã€‚é‚£æ˜¯ä¸ª LTI ç³»ç»Ÿã€‚\n",
    "\n",
    "ä½†å¦‚æœè¿™è¾†è½¦åœ¨è¡Œé©¶è¿‡ç¨‹ä¸­ä¸æ–­å‡è½»ï¼ˆæ¯”å¦‚ç‡ƒæ²¹æ¶ˆè€—æˆ–è½½é‡å˜åŒ–ï¼‰ï¼Œé‚£å®ƒçš„å“åº”ç‰¹æ€§ä¼šå˜ã€‚è¿™ä¸ªç³»ç»Ÿçš„â€œè´¨é‡â€å°±æ˜¯éšæ—¶é—´å˜åŒ–çš„å‚æ•°ï¼Œæ‰€ä»¥å®ƒå°±æ˜¯ä¸€ä¸ª **LTV ç³»ç»Ÿ**ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”„ LTI vs LTV å¯¹æ¯”è¡¨\n",
    "\n",
    "| ç‰¹æ€§         | LTI ç³»ç»Ÿ                    | LTV ç³»ç»Ÿ                     |\n",
    "|--------------|-----------------------------|------------------------------|\n",
    "| å‚æ•°æ˜¯å¦å˜   | ä¸å˜ï¼ˆå¸¸æ•°ï¼‰                | éšæ—¶é—´å˜åŒ–                   |\n",
    "| è§£çš„éš¾åº¦     | å®¹æ˜“ï¼Œæœ‰å°é—­è§£               | è¾ƒéš¾ï¼Œé€šå¸¸éœ€è¦æ•°å€¼è§£æ³•       |\n",
    "| å¸¸è§æ±‚è§£æ–¹æ³• | çŸ©é˜µæŒ‡æ•°ã€æ‹‰æ™®æ‹‰æ–¯å˜æ¢ç­‰    | æ•°å€¼ç§¯åˆ†ã€å˜æ¢åŸºç­‰           |\n",
    "| å®ä¾‹         | ç”µè·¯ã€ç”µæœºã€æ»¤æ³¢å™¨ç­‰         | é£æœºã€èˆªå¤©å™¨ã€æœºå™¨äººç­‰åŠ¨æ€å˜åŒ–ç³»ç»Ÿ |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸš€ ä¸ºä»€ä¹ˆè¦å­¦ LTV ç³»ç»Ÿï¼Ÿ\n",
    "\n",
    "- âœ… æ›´è´´è¿‘çœŸå®ä¸–ç•Œï¼šå¾ˆå¤šç³»ç»Ÿå‚æ•°éƒ½ä¼šéšæ—¶é—´å˜åŒ–ï¼Œæ¯”å¦‚é£æœºé£è¡Œè¿‡ç¨‹ä¸­çš„é‡é‡ã€ç”µæ± ç”µé‡å˜åŒ–ç­‰\n",
    "- âœ…\n",
    "\n",
    "\n",
    "### ğŸ§  ä¸ºä»€ä¹ˆçŸ©é˜µæŒ‡æ•° $e^{\\mathbf{A}t}$ åªèƒ½ç”¨äº LTI ç³»ç»Ÿï¼Ÿ\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… é¦–å…ˆï¼Œä»€ä¹ˆæ˜¯çŸ©é˜µæŒ‡æ•°ï¼Ÿ\n",
    "\n",
    "å¯¹äºä¸€ä¸ª LTI ç³»ç»Ÿï¼š\n",
    "\n",
    "$$\n",
    "\\dot{\\mathbf{x}}(t) = \\mathbf{A} \\mathbf{x}(t)\n",
    "$$\n",
    "\n",
    "æˆ‘ä»¬å¯ä»¥å†™å‡ºå®ƒçš„è§£ä¸ºï¼š\n",
    "\n",
    "$$\n",
    "\\mathbf{x}(t) = e^{\\mathbf{A}t} \\mathbf{x}(0)\n",
    "$$\n",
    "\n",
    "å…¶ä¸­çŸ©é˜µæŒ‡æ•°çš„å®šä¹‰ä¸ºï¼š\n",
    "\n",
    "$$\n",
    "e^{\\mathbf{A}t} = \\sum_{k=0}^{\\infty} \\frac{(\\mathbf{A}t)^k}{k!}\n",
    "$$\n",
    "\n",
    "å®ƒæ˜¯ä¸€ä¸ª**å°é—­å½¢å¼çš„è§£æè§£**ï¼Œç”¨äºæè¿°ç³»ç»ŸçŠ¶æ€éšæ—¶é—´æ¼”åŒ–çš„è¿‡ç¨‹ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ›‘ ä¸ºä»€ä¹ˆè¿™ä¸ªæ–¹æ³•**ä¸èƒ½ç”¨äº LTV ç³»ç»Ÿ**ï¼Ÿ\n",
    "\n",
    "å¦‚æœæˆ‘ä»¬ç°åœ¨æœ‰ä¸€ä¸ª **çº¿æ€§æ—¶å˜ç³»ç»Ÿï¼ˆLTVï¼‰**ï¼š\n",
    "\n",
    "$$\n",
    "\\dot{\\mathbf{x}}(t) = \\mathbf{A}(t) \\mathbf{x}(t)\n",
    "$$\n",
    "\n",
    "ä½ ä¼šå‘ç°è¿™ä¸ªæ–¹ç¨‹æ›´å¤æ‚ï¼Œå› ä¸ºï¼š\n",
    "\n",
    "- ç³»ç»ŸçŸ©é˜µ $\\mathbf{A}(t)$ æ˜¯**éšæ—¶é—´å˜åŒ–çš„å‡½æ•°**\n",
    "- é‚£ä¹ˆåœ¨ä¸åŒæ—¶é—´ï¼Œç³»ç»Ÿçš„è¡Œä¸ºå®Œå…¨ä¸åŒï¼ˆä¸æ˜¯å¸¸æ•°çŸ©é˜µï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "### âŒ ä¸ºä»€ä¹ˆä¸èƒ½ç”¨ $e^{\\mathbf{A}(t)t}$ï¼Ÿ\n",
    "\n",
    "ä½ å¯èƒ½ä¼šé—®ï¼šé‚£æˆ‘ä»¬å†™æˆï¼š\n",
    "\n",
    "$$\n",
    "\\mathbf{x}(t) = e^{\\mathbf{A}(t)t} \\mathbf{x}(0)\n",
    "$$\n",
    "\n",
    "è¡Œä¸è¡Œï¼Ÿ\n",
    "\n",
    "ç­”æ¡ˆæ˜¯ **ä¸è¡Œï¼**\n",
    "\n",
    "å› ä¸ºçŸ©é˜µæŒ‡æ•°çš„å®šä¹‰æ˜¯åŸºäºå¸¸æ•°çŸ©é˜µçš„ï¼š\n",
    "\n",
    "$$\n",
    "\\frac{d}{dt} \\left( e^{\\mathbf{A}t} \\right) = \\mathbf{A} e^{\\mathbf{A}t}\n",
    "$$\n",
    "\n",
    "ä½†å¦‚æœ $\\mathbf{A}$ æ˜¯ä¸€ä¸ªå‡½æ•° $\\mathbf{A}(t)$ï¼Œé‚£å¯¼æ•°å°±ä¸æ˜¯ç®€å•çš„ï¼š\n",
    "\n",
    "$$\n",
    "\\frac{d}{dt} \\left( e^{\\mathbf{A}(t)t} \\right) \\neq \\mathbf{A}(t) e^{\\mathbf{A}(t)t}\n",
    "$$\n",
    "\n",
    "å®é™…ä¸Šï¼Œå®ƒåŒ…å«ä¸€ä¸ªéå¸¸å¤æ‚çš„æ±‚å¯¼é“¾å¼è§„åˆ™ï¼Œç”šè‡³å¯èƒ½**æ— æ³•ç”¨å°é—­å½¢å¼è¡¨è¾¾**ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”§ é‚£ä¹ˆ LTV ç³»ç»Ÿæ€ä¹ˆæ±‚è§£ï¼Ÿ\n",
    "\n",
    "LTV ç³»ç»Ÿçš„ä¸€èˆ¬è§£æ˜¯ç”¨æ‰€è°“çš„ **çŠ¶æ€è½¬ç§»çŸ©é˜µ $\\Phi(t, t_0)$**ï¼š\n",
    "\n",
    "$$\n",
    "\\mathbf{x}(t) = \\Phi(t, t_0) \\mathbf{x}(t_0)\n",
    "$$\n",
    "\n",
    "è¿™ä¸ª $\\Phi$ **ä¸ç­‰äº** ç®€å•çš„ $e^{\\mathbf{A}(t)(t - t_0)}$ï¼Œè€Œæ˜¯é€šè¿‡**æ—¶é—´æœ‰åºç§¯åˆ†**å¾—åˆ°çš„ï¼š\n",
    "\n",
    "$$\n",
    "\\Phi(t, t_0) = \\mathcal{T} \\exp\\left( \\int_{t_0}^t \\mathbf{A}(\\tau) d\\tau \\right)\n",
    "$$\n",
    "\n",
    "å…¶ä¸­ $\\mathcal{T}$ æ˜¯**æ—¶é—´æœ‰åºç®—ç¬¦**ï¼Œè¡¨ç¤ºä¸åŒæ—¶é—´ç‚¹çš„çŸ©é˜µä¹˜æ³•**é¡ºåº**è¦è€ƒè™‘åœ¨å†…ï¼Œè¿™å°±éå¸¸å¤æ‚ï¼Œé€šå¸¸æ²¡æœ‰è§£æè§£ï¼Œåªèƒ½ç”¨æ•°å€¼æ–¹æ³•æ±‚è§£ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… æ€»ç»“ä¸€å¥è¯ï¼š\n",
    "\n",
    "> çŸ©é˜µæŒ‡æ•° $e^{\\mathbf{A}t}$ åªèƒ½ç”¨äº LTI ç³»ç»Ÿï¼Œæ˜¯å› ä¸ºå®ƒå‡è®¾ $\\mathbf{A}$ æ˜¯å¸¸æ•°çŸ©é˜µã€‚è€Œåœ¨ LTV ç³»ç»Ÿä¸­ï¼Œ$\\mathbf{A}(t)$ éšæ—¶é—´å˜åŒ–ï¼Œå¯¼è‡´çŸ©é˜µæŒ‡æ•°çš„æ€§è´¨ä¸å†æˆç«‹ï¼Œå¿…é¡»ä½¿ç”¨çŠ¶æ€è½¬ç§»çŸ©é˜µ $\\Phi(t, t_0)$ å’Œæ•°å€¼æ–¹æ³•æ¥æ±‚è§£ã€‚\n",
    "\n",
    "\n",
    "### Numerical Solutions\n",
    "\n",
    "Finally, there are numerical techniques to find $\\mathbf F$. As filters get larger finding analytical solutions becomes very tedious (though packages like SymPy make it easier). C. F. van Loan [2] has developed a technique that finds both $\\Phi$ and $\\mathbf Q$ numerically. Given the continuous model\n",
    "\n",
    "$$ \\dot x = Ax + Gw$$\n",
    "\n",
    "where $w$ is the unity white noise, van Loan's method computes both $\\mathbf F_k$ and $\\mathbf Q_k$.\n",
    "    \n",
    "I have implemented van Loan's method in `FilterPy`. You may use it as follows:\n",
    "\n",
    "```python\n",
    "from filterpy.common import van_loan_discretization\n",
    "\n",
    "A = np.array([[0., 1.], [-1., 0.]])\n",
    "G = np.array([[0.], [2.]]) # white noise scaling\n",
    "F, Q = van_loan_discretization(A, G, dt=0.1)\n",
    "```\n",
    "    \n",
    "In the section *Numeric Integration of Differential Equations* I present alternative methods which are very commonly used in Kalman filtering.\n",
    "\n",
    "## Design of the Process Noise Matrix\n",
    "\n",
    "In general the design of the $\\mathbf Q$ matrix is among the most difficult aspects of Kalman filter design. This is due to several factors. First, the math requires a good foundation in signal theory. Second, we are trying to model the noise in something for which we have little information. Consider trying to model the process noise for a thrown baseball. We can model it as a sphere moving through the air, but that leaves many unknown factors - ball rotation and spin decay, the coefficient of drag of a ball with stitches, the effects of wind and air density, and so on. We develop the equations for an exact mathematical solution for a given process model, but since the process model is incomplete the result for $\\mathbf Q$ will also be incomplete. This has a lot of ramifications for the behavior of the Kalman filter. If $\\mathbf Q$ is too small then the filter will be overconfident in its prediction model and will diverge from the actual solution. If $\\mathbf Q$ is too large than the filter will be unduly influenced by the noise in the measurements and perform sub-optimally. In practice we spend a lot of time running simulations and evaluating collected data to try to select an appropriate value for $\\mathbf Q$. But let's start by looking at the math.\n",
    "\n",
    "\n",
    "Let's assume a kinematic system - some system that can be modeled using Newton's equations of motion. We can make a few different assumptions about this process. \n",
    "\n",
    "We have been using a process model of\n",
    "\n",
    "$$ \\dot{\\mathbf x} = \\mathbf{Ax} + \\mathbf{Bu} + \\mathbf{w}$$\n",
    "\n",
    "where $\\mathbf{w}$ is the process noise. Kinematic systems are *continuous* - their inputs and outputs can vary at any arbitrary point in time. However, our Kalman filters are *discrete* (there are continuous forms for Kalman filters, but we do not cover them in this book). We sample the system at regular intervals. Therefore we must find the discrete representation for the noise term in the equation above. This depends on what assumptions we make about the behavior of the noise. We will consider two different models for the noise.\n",
    "\n",
    "### Continuous White Noise Model\n",
    "\n",
    "We model kinematic systems using Newton's equations. We have either used position and velocity, or position, velocity, and acceleration as the models for our systems. There is nothing stopping us from going further - we can model jerk, jounce, snap, and so on. We don't do that normally because adding terms beyond the dynamics of the real system degrades the estimate. \n",
    "\n",
    "Let's say that we need to model the position, velocity, and acceleration. We can then assume that acceleration is constant for each discrete time step. Of course, there is process noise in the system and so the acceleration is not actually constant. The tracked object will alter the acceleration over time due to external, unmodeled forces. In this section we will assume that the acceleration changes by a continuous time zero-mean white noise $w(t)$. In other words, we are assuming that the small changes in velocity average to 0 over time (zero-mean). \n",
    "\n",
    "Since the noise is changing continuously we will need to integrate to get the discrete noise for the discretization interval that we have chosen. We will not prove it here, but the equation for the discretization of the noise is\n",
    "\n",
    "$$\\mathbf Q = \\int_0^{\\Delta t} \\mathbf F(t)\\mathbf{Q_c}\\mathbf F^\\mathsf{T}(t) dt$$\n",
    "\n",
    "where $\\mathbf{Q_c}$ is the continuous noise. The general reasoning should be clear. $\\mathbf F(t)\\mathbf{Q_c}\\mathbf F^\\mathsf{T}(t)$ is a projection of the continuous noise based on our process model $\\mathbf F(t)$ at the instant $t$. We want to know how much noise is added to the system over a discrete intervat $\\Delta t$, so we integrate this expression over the interval $[0, \\Delta t]$. \n",
    "\n",
    "We know the fundamental matrix for Newtonian systems is\n",
    "\n",
    "$$F = \\begin{bmatrix}1 & \\Delta t & {\\Delta t}^2/2 \\\\ 0 & 1 & \\Delta t\\\\ 0& 0& 1\\end{bmatrix}$$\n",
    "\n",
    "We define the continuous noise as \n",
    "\n",
    "$$\\mathbf{Q_c} = \\begin{bmatrix}0&0&0\\\\0&0&0\\\\0&0&1\\end{bmatrix} \\Phi_s$$\n",
    "\n",
    "where $\\Phi_s$ is the spectral density of the white noise. This can be derived, but is beyond the scope of this book. See any standard text on stochastic processes for the details. In practice we often do not know the spectral density of the noise, and so this turns into an \"engineering\" factor - a number we experimentally tune until our filter performs as we expect. You can see that the matrix that $\\Phi_s$ is multiplied by effectively assigns the power spectral density to the acceleration term. This makes sense; we assume that the system has constant acceleration except for the variations caused by noise. The noise alters the acceleration.\n",
    "\n",
    "We could carry out these computations ourselves, but I prefer using SymPy to solve the equation.\n",
    "\n",
    "$$\\mathbf{Q_c} = \\begin{bmatrix}0&0&0\\\\0&0&0\\\\0&0&1\\end{bmatrix} \\Phi_s$$\n",
    "\n",
    "\n",
    "\n",
    "import sympy\n",
    "from sympy import (init_printing, Matrix, MatMul, \n",
    "                   integrate, symbols)\n",
    "\n",
    "init_printing(use_latex='mathjax')\n",
    "dt, phi = symbols('\\Delta{t} \\Phi_s')\n",
    "F_k = Matrix([[1, dt, dt**2/2],\n",
    "              [0,  1,      dt],\n",
    "              [0,  0,       1]])\n",
    "Q_c = Matrix([[0, 0, 0],\n",
    "              [0, 0, 0],\n",
    "              [0, 0, 1]])*phi\n",
    "\n",
    "Q = integrate(F_k * Q_c * F_k.T, (dt, 0, dt))\n",
    "\n",
    "# factor phi out of the matrix to make it more readable\n",
    "Q = Q / phi\n",
    "MatMul(Q, phi)\n",
    "\n",
    "For completeness, let us compute the equations for the 0th order and 1st order equations.\n",
    "\n",
    "F_k = Matrix([[1]])\n",
    "Q_c = Matrix([[phi]])\n",
    "\n",
    "print('0th order discrete process noise')\n",
    "integrate(F_k*Q_c*F_k.T,(dt, 0, dt))\n",
    "\n",
    "F_k = Matrix([[1, dt],\n",
    "              [0, 1]])\n",
    "Q_c = Matrix([[0, 0],\n",
    "              [0, 1]]) * phi\n",
    "\n",
    "Q = integrate(F_k * Q_c * F_k.T, (dt, 0, dt))\n",
    "\n",
    "print('1st order discrete process noise')\n",
    "# factor phi out of the matrix to make it more readable\n",
    "Q = Q / phi\n",
    "MatMul(Q, phi)\n",
    "\n",
    "### Piecewise White Noise Model\n",
    "\n",
    "Another model for the noise assumes that the that highest order term (say, acceleration) is constant for the duration of each time period, but differs for each time period, and each of these is uncorrelated between time periods. In other words there is a discontinuous jump in acceleration at each time step. This is subtly different than the model above, where we assumed that the last term had a continuously varying noisy signal applied to it.  \n",
    "\n",
    "We will model this as\n",
    "\n",
    "$$f(x)=Fx+\\Gamma w$$\n",
    "\n",
    "where $\\Gamma$ is the *noise gain* of the system, and $w$ is the constant piecewise acceleration (or velocity, or jerk, etc). \n",
    "\n",
    "Let's start by looking at a first order system. In this case we have the state transition function\n",
    "\n",
    "$$\\mathbf{F} = \\begin{bmatrix}1&\\Delta t \\\\ 0& 1\\end{bmatrix}$$\n",
    "\n",
    "In one time period, the change in velocity will be $w(t)\\Delta t$, and the change in position will be $w(t)\\Delta t^2/2$, giving us\n",
    "\n",
    "$$\\Gamma = \\begin{bmatrix}\\frac{1}{2}\\Delta t^2 \\\\ \\Delta t\\end{bmatrix}$$\n",
    "\n",
    "The covariance of the process noise is then\n",
    "\n",
    "$$Q = \\mathbb E[\\Gamma w(t) w(t) \\Gamma^\\mathsf{T}] = \\Gamma\\sigma^2_v\\Gamma^\\mathsf{T}$$.\n",
    "\n",
    "We can compute that with SymPy as follows\n",
    "\n",
    "var = symbols('sigma^2_v')\n",
    "v = Matrix([[dt**2 / 2], [dt]])\n",
    "\n",
    "Q = v * var * v.T\n",
    "\n",
    "# factor variance out of the matrix to make it more readable\n",
    "Q = Q / var\n",
    "MatMul(Q, var)\n",
    "\n",
    "The second order system proceeds with the same math.\n",
    "\n",
    "\n",
    "$$\\mathbf{F} = \\begin{bmatrix}1 & \\Delta t & {\\Delta t}^2/2 \\\\ 0 & 1 & \\Delta t\\\\ 0& 0& 1\\end{bmatrix}$$\n",
    "\n",
    "Here we will assume that the white noise is a discrete time Wiener process. This gives us\n",
    "\n",
    "$$\\Gamma = \\begin{bmatrix}\\frac{1}{2}\\Delta t^2 \\\\ \\Delta t\\\\ 1\\end{bmatrix}$$\n",
    "\n",
    "There is no 'truth' to this model, it is just convenient and provides good results. For example, we could assume that the noise is applied to the jerk at the cost of a more complicated equation. \n",
    "\n",
    "The covariance of the process noise is then\n",
    "\n",
    "$$Q = \\mathbb E[\\Gamma w(t) w(t) \\Gamma^\\mathsf{T}] = \\Gamma\\sigma^2_v\\Gamma^\\mathsf{T}$$.\n",
    "\n",
    "We can compute that with SymPy as follows\n",
    "\n",
    "var = symbols('sigma^2_v')\n",
    "v = Matrix([[dt**2 / 2], [dt], [1]])\n",
    "\n",
    "Q = v * var * v.T\n",
    "\n",
    "# factor variance out of the matrix to make it more readable\n",
    "Q = Q / var\n",
    "MatMul(Q, var)\n",
    "\n",
    "We cannot say that this model is more or less correct than the continuous model - both are approximations to what is happening to the actual object. Only experience and experiments can guide you to the appropriate model. In practice you will usually find that either model provides reasonable results, but typically one will perform better than the other.\n",
    "\n",
    "The advantage of the second model is that we can model the noise in terms of $\\sigma^2$ which we can describe in terms of the motion and the amount of error we expect. The first model requires us to specify the spectral density, which is not very intuitive, but it handles varying time samples much more easily since the noise is integrated across the time period. However, these are not fixed rules - use whichever model (or a model of your own devising) based on testing how the filter performs and/or your knowledge of the behavior of the physical model.\n",
    "\n",
    "A good rule of thumb is to set $\\sigma$ somewhere from $\\frac{1}{2}\\Delta a$ to $\\Delta a$, where $\\Delta a$ is the maximum amount that the acceleration will change between sample periods. In practice we pick a number, run simulations on data, and choose a value that works well.\n",
    "\n",
    "### Using FilterPy to Compute Q\n",
    "\n",
    "FilterPy offers several routines to compute the $\\mathbf Q$ matrix. The function `Q_continuous_white_noise()` computes $\\mathbf Q$ for a given value for $\\Delta t$ and the spectral density.\n",
    "\n",
    "from filterpy.common import Q_continuous_white_noise\n",
    "from filterpy.common import Q_discrete_white_noise\n",
    "\n",
    "Q = Q_continuous_white_noise(dim=2, dt=1, spectral_density=1)\n",
    "print(Q)\n",
    "\n",
    "Q = Q_continuous_white_noise(dim=3, dt=1, spectral_density=1)\n",
    "print(Q)\n",
    "\n",
    "The function `Q_discrete_white_noise()` computes $\\mathbf Q$ assuming a piecewise model for the noise.\n",
    "\n",
    "Q = Q_discrete_white_noise(2, var=1.)\n",
    "print(Q)\n",
    "\n",
    "Q = Q_discrete_white_noise(3, var=1.)\n",
    "print(Q)\n",
    "\n",
    "### Simplification of Q\n",
    "\n",
    "Many treatments use a much simpler form for $\\mathbf Q$, setting it to zero except for a noise term in the lower rightmost element. Is this justified? Well, consider the value of $\\mathbf Q$ for a small $\\Delta t$\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(precision=8)\n",
    "Q = Q_continuous_white_noise(\n",
    "    dim=3, dt=0.05, spectral_density=1)\n",
    "print(Q)\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "We can see that most of the terms are very small. Recall that the only equation using this matrix is\n",
    "\n",
    "$$ \\mathbf P=\\mathbf{FPF}^\\mathsf{T} + \\mathbf Q$$\n",
    "\n",
    "If the values for $\\mathbf Q$ are small relative to $\\mathbf P$\n",
    "then it will be contributing almost nothing to the computation of $\\mathbf P$. Setting $\\mathbf Q$ to the zero matrix except for the lower right term\n",
    "\n",
    "$$\\mathbf Q=\\begin{bmatrix}0&0&0\\\\0&0&0\\\\0&0&\\sigma^2\\end{bmatrix}$$\n",
    "\n",
    "while not correct, is often a useful approximation. If you do this for an important application you will have to perform quite a few studies to guarantee that your filter works in a variety of situations. \n",
    "\n",
    "If you do this, 'lower right term' means the most rapidly changing term for each variable. If the state is $x=\\begin{bmatrix}x & \\dot x & \\ddot{x} & y & \\dot{y} & \\ddot{y}\\end{bmatrix}^\\mathsf{T}$ Then $\\mathbf Q$ will be 6x6; the elements for both $\\ddot{x}$ and $\\ddot{y}$ will have to be set to non-zero in $\\mathbf Q$.\n",
    "\n",
    "## Stable Compution of the Posterior Covariance\n",
    "\n",
    "I've presented the equation to compute the posterior covariance as\n",
    "\n",
    "$$\\mathbf P = (\\mathbf I - \\mathbf{KH})\\mathbf{\\bar P}$$\n",
    "\n",
    "and while strictly speaking this is correct, this is not how I compute it in `FilterPy`, where I use the *Joseph* equation\n",
    "\n",
    "\n",
    "$$\\mathbf P = (\\mathbf I-\\mathbf {KH})\\mathbf{\\bar P}(\\mathbf I-\\mathbf{KH})^\\mathsf T + \\mathbf{KRK}^\\mathsf T$$\n",
    "\n",
    "\n",
    "I frequently get emails and/or GitHub issues raised, claiming the implementation is a bug. It is not a bug, and I use it for several reasons. First, the subtraction $(\\mathbf I - \\mathbf{KH})$ can lead to nonsymmetric matrix results due to floating point errors. Covariances must be symmetric, and so becoming nonsymmetric usually leads to the Kalman filter diverging, or even for the code to raise an exception because of the checks built into `NumPy`.\n",
    "\n",
    "A traditional way to preserve symmetry is the following formula:\n",
    "\n",
    "$$\\mathbf P = (\\mathbf P + \\mathbf P^\\mathsf T) / 2$$\n",
    "\n",
    "This is safe because $\\sigma_{ij} = \\sigma_{ji}$ for all covariances in the matrix. Hence this operation averages the error between the differences of the two values if they have diverged due to floating point errors. \n",
    "\n",
    "If you look at the Joseph form for the equation above, you'll see there is a similar $\\mathbf{ABA}^\\mathsf T$ pattern in both terms. So they both preserve symmetry. But where did this equation come from, and why do I use it instead of\n",
    "\n",
    "\n",
    "$$\\mathbf P = (\\mathbf I - \\mathbf{KH})\\mathbf{\\bar P} \\\\\n",
    "\\mathbf P = (\\mathbf P + \\mathbf P^\\mathsf T) / 2$$\n",
    "\n",
    "\n",
    "Let's just derive the equation from first principles. It's not too bad, and you need to understand the derivation to understand the purpose of the equation, and, more importantly, diagnose issues if your filter diverges due to numerical instability. This derivation comes from Brown[4].\n",
    "\n",
    "First, some symbology. $\\mathbf x$ is the true state of our system. $\\mathbf{\\hat x}$ is the estimated state of our system - the posterior. And $\\mathbf{\\bar x}$ is the estimated prior of the system. \n",
    "\n",
    "\n",
    "Given that, we can define our model to be\n",
    "\n",
    "$$\\mathbf x_{k+1} = \\mathbf F_k \\mathbf x_k + \\mathbf w_k \\\\\n",
    "\\mathbf z_k = \\mathbf H_k \\mathbf x_k + \\mathbf v_k$$\n",
    "\n",
    "In words, the next state $\\mathbf x_{k+1}$ of the system is the current state $k$ moved by some process $\\mathbf F_k$ plus some noise $\\mathbf w_k$. \n",
    "\n",
    "Note that these are definitions. No system perfectly follows a mathematical model, so we model that with the noise term $\\mathbf w_k$. And no measurement is perfect due to sensor error, so we model that with $\\mathbf v_k$\n",
    "\n",
    "I'll dispense with the subscript $k$ since in the remainder of the derivation we will only consider values at step $k$, never step $k+1$.\n",
    "\n",
    "Now we define the estimation error as the difference between the true state and the estimated state\n",
    "\n",
    "$$ \\mathbf e = \\mathbf x - \\mathbf{\\hat x}$$\n",
    "\n",
    "Again, this is a definition; we don't know how to compute $\\mathbf e$, it is just the defined difference between the true and estimated state.\n",
    "\n",
    "This allows us to define the covariance of our estimate, which is defined as the expected value of $\\mathbf{ee}^\\mathsf T$:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "P &= E[\\mathbf{ee}^\\mathsf T] \\\\\n",
    "&= E[(\\mathbf x - \\mathbf{\\hat x})(\\mathbf x - \\mathbf{\\hat x})^\\mathsf T]\n",
    "\\end{aligned}$$\n",
    "\n",
    "\n",
    "Next, we define the posterior estimate as\n",
    "\n",
    "$$\\mathbf {\\hat x} = \\mathbf{\\bar x} + \\mathbf K(\\mathbf z - \\mathbf{H \\bar x})$$\n",
    "\n",
    "That looks like the equation from the Kalman filter, and for good reason. But as with the rest of the math so far, this is a **definition**. In particular, we have not defined $\\mathbf K$, and you shouldn't think of it as the Kalman gain, because we are solving this for *any* problem, not just for linear Kalman filters. Here, $\\mathbf K$ is just some unspecified blending value between 0 and 1.\n",
    "\n",
    "Now we have our definitions, let's perform some substitution and algebra.\n",
    "\n",
    "The term $(\\mathbf x - \\mathbf{\\hat x})$ can be expanded by replacing $\\mathbf{\\hat x}$ with the definition above, yielding\n",
    "\n",
    "$$(\\mathbf x - \\mathbf{\\hat x}) = \\mathbf x - (\\mathbf{\\bar x} + \\mathbf K(\\mathbf z - \\mathbf{H \\bar x}))$$\n",
    "\n",
    "Now we replace $\\mathbf z$ with $\\mathbf H \\mathbf x + \\mathbf v$:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "(\\mathbf x - \\mathbf{\\hat x})\n",
    "&= \\mathbf x - (\\mathbf{\\bar x} + \\mathbf K(\\mathbf z - \\mathbf{H \\bar x})) \\\\\n",
    "&= \\mathbf x - (\\mathbf{\\bar x} + \\mathbf K(\\mathbf H \\mathbf x + \\mathbf v - \\mathbf{H \\bar x})) \\\\\n",
    "&= (\\mathbf x - \\mathbf{\\bar x}) - \\mathbf K(\\mathbf H \\mathbf x + \\mathbf v - \\mathbf{H \\bar x}) \\\\\n",
    "&= (\\mathbf x - \\mathbf{\\bar x}) - \\mathbf{KH}(\\mathbf x - \\mathbf{ \\bar x}) - \\mathbf{Kv} \\\\\n",
    "&=  (\\mathbf I - \\mathbf{KH})(\\mathbf x - \\mathbf{\\bar x}) - \\mathbf{Kv}\n",
    "\\end{aligned}$$\n",
    "\n",
    "Now we can solve for $\\mathbf P$ if we note that the expected value of $(\\mathbf x - \\mathbf{\\bar x})$ is the prior covariance $\\mathbf{\\bar P}$, and that the expected value of $\\mathbf v$ is $E[\\mathbf{vv}^\\mathbf T] = \\mathbf R$:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbf P &= \n",
    "   E\\big[[(\\mathbf I - \\mathbf{KH})(\\mathbf x - \\mathbf{\\bar x}) - \\mathbf{Kv})]\n",
    "  [(\\mathbf I - \\mathbf{KH})(\\mathbf x - \\mathbf{\\bar x}) - \\mathbf{Kv}]^\\mathsf T\\big ] \\\\\n",
    "  &= (\\mathbf I - \\mathbf{KH})\\mathbf{\\bar P}(\\mathbf I - \\mathbf{KH})^\\mathsf T + \\mathbf{KRK}^\\mathsf T\n",
    "\\end{aligned}$$\n",
    "\n",
    "which is what we came here to prove.\n",
    "\n",
    "Note that this equation is valid for *any* $\\mathbf K$, not just the optimal $\\mathbf K$ computed by the Kalman filter. And that is why I use this equation. In practice the Kalman gain computed by the filter is *not* the optimal value both because the real world is never truly linear and Gaussian, and because of floating point errors induced by computation. This equation is far less likely to cause the Kalman filter to diverge in the face of real world conditions.\n",
    "\n",
    "Where did $\\mathbf P = (\\mathbf I - \\mathbf{KH})\\mathbf{\\bar P}$ come from, then? Let's finish the derivation, which is simple. Recall that the Kalman filter (optimal) gain is given by\n",
    "\n",
    "$$\\mathbf K = \\mathbf{\\bar P H^\\mathsf T}(\\mathbf{H \\bar P H}^\\mathsf T + \\mathbf R)^{-1}$$\n",
    "\n",
    "Now we substitute this into the equation we just derived:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "&= (\\mathbf I - \\mathbf{KH})\\mathbf{\\bar P}(\\mathbf I - \\mathbf{KH})^\\mathsf T + \\mathbf{KRK}^\\mathsf T\\\\\n",
    "&= \\mathbf{\\bar P} - \\mathbf{KH}\\mathbf{\\bar P} - \\mathbf{\\bar PH}^\\mathsf T\\mathbf{K}^\\mathsf T + \\mathbf K(\\mathbf{H \\bar P H}^\\mathsf T + \\mathbf R)\\mathbf K^\\mathsf T \\\\\n",
    "&= \\mathbf{\\bar P} - \\mathbf{KH}\\mathbf{\\bar P} - \\mathbf{\\bar PH}^\\mathsf T\\mathbf{K}^\\mathsf T + \\mathbf{\\bar P H^\\mathsf T}(\\mathbf{H \\bar P H}^\\mathsf T + \\mathbf R)^{-1}(\\mathbf{H \\bar P H}^\\mathsf T + \\mathbf R)\\mathbf K^\\mathsf T\\\\\n",
    "&= \\mathbf{\\bar P} - \\mathbf{KH}\\mathbf{\\bar P} - \\mathbf{\\bar PH}^\\mathsf T\\mathbf{K}^\\mathsf T + \\mathbf{\\bar P H^\\mathsf T}\\mathbf K^\\mathsf T\\\\\n",
    "&= \\mathbf{\\bar P} - \\mathbf{KH}\\mathbf{\\bar P}\\\\\n",
    "&= (\\mathbf I - \\mathbf{KH})\\mathbf{\\bar P}\n",
    "\\end{aligned}$$\n",
    "\n",
    "Therefore $\\mathbf P = (\\mathbf I - \\mathbf{KH})\\mathbf{\\bar P}$ is mathematically correct when the gain is optimal, but so is $(\\mathbf I - \\mathbf{KH})\\mathbf{\\bar P}(\\mathbf I - \\mathbf{KH})^\\mathsf T + \\mathbf{KRK}^\\mathsf T$. As we already discussed the latter is also correct when the gain is suboptimal, and it is also more numerically stable. Therefore I use this computation in FilterPy.\n",
    "\n",
    "It is quite possible that your filter still diverges, especially if it runs for hundreds or thousands of epochs. You will need to examine these equations. The literature provides yet other forms of this computation which may be more applicable to your problem. As always, if you are solving real engineering problems where failure could mean loss of equipment or life, you will need to move past this book and into the engineering literature. If you are working with 'toy' problems where failure is not damaging, if you detect divergence you can just reset the value of $\\mathbf P$ to some 'reasonable' value and keep on going. For example, you could zero out the non diagonal elements so the matrix only contains variances, and then maybe multiply by a constant somewhat larger than one to reflect the loss of information you just injected into the filter. Use your imagination, and test.\n",
    "\n",
    "## Deriving the Kalman Gain Equation\n",
    "\n",
    "If you read the last section, you might as well read this one. With this we will have derived the Kalman filter equations.\n",
    "\n",
    "Note that this derivation is *not* using Bayes equations. I've seen at least four different ways to derive the Kalman filter equations; this derivation is typical to the literature, and follows from the last section. The source is again Brown [4].\n",
    "\n",
    "In the last section we used an unspecified scaling factor $\\mathbf K$ to derive the Joseph form of the covariance equation. If we want an optimal filter, we need to use calculus to minimize the errors in the equations. You should be familiar with this idea. If you want to find the minimum value of a function $f(x)$, you take the derivative and set it equal to zero: $\\frac{x}{dx}f(x) = 0$.\n",
    "\n",
    "In our problem the error is expressed by the covariance matrix $\\mathbf P$. In particular, the diagonal expresses the error (variance) of each element in the state vector. So, to find the optimal gain we want to take the derivative of the trace (sum) of the diagonal.\n",
    "\n",
    "Brown reminds us of two formulas involving the derivative of traces:\n",
    "\n",
    "$$\\frac{d\\, trace(\\mathbf{AB})}{d\\mathbf A} = \\mathbf B^\\mathsf T$$\n",
    "\n",
    "$$\\frac{d\\, trace(\\mathbf{ACA}^\\mathsf T)}{d\\mathbf A} = 2\\mathbf{AC}$$\n",
    "\n",
    "where $\\mathbf{AB}$ is square and $\\mathbf C$ is symmetric.\n",
    "\n",
    "\n",
    "We expand out the Joseph equation to:\n",
    "\n",
    "$$\\mathbf P = \\mathbf{\\bar P} - \\mathbf{KH}\\mathbf{\\bar P} - \\mathbf{\\bar P}\\mathbf H^\\mathsf T \\mathbf K^\\mathsf T + \\mathbf K(\\mathbf H \\mathbf{\\bar P}\\mathbf H^\\mathsf T + \\mathbf R)\\mathbf K^\\mathsf T$$\n",
    "\n",
    "Now we need to the the derivative of the trace of $\\mathbf P$ with respect to $\\mathbf K$: $\\frac{d\\, trace(\\mathbf P)}{d\\mathbf K}$.\n",
    "\n",
    "The derivative of the trace the first term with respect to $\\mathbf K$ is $0$, since it does not have $\\mathbf K$ in the expression.\n",
    "\n",
    "The derivative of the trace of the second term is $(\\mathbf H\\mathbf{\\bar P})^\\mathsf T$.\n",
    "\n",
    "We can find the derivative of the trace of the third term by noticing that $\\mathbf{\\bar P}\\mathbf H^\\mathsf T \\mathbf K^\\mathsf T$ is the transpose of $\\mathbf{KH}\\mathbf{\\bar P}$. The trace of a matrix is equal to the trace of it's transpose, so it's derivative will be same as the second term.\n",
    "\n",
    "Finally, the derivative of the trace of the fourth term is $2\\mathbf K(\\mathbf H \\mathbf{\\bar P}\\mathbf H^\\mathsf T + \\mathbf R)$.\n",
    "\n",
    "This gives us the final value of \n",
    "\n",
    "$$\\frac{d\\, trace(\\mathbf P)}{d\\mathbf K} = -2(\\mathbf H\\mathbf{\\bar P})^\\mathsf T + 2\\mathbf K(\\mathbf H \\mathbf{\\bar P}\\mathbf H^\\mathsf T + \\mathbf R)$$\n",
    "\n",
    "We set this to zero and solve to find the equation for $\\mathbf K$ which minimizes the error:\n",
    "\n",
    "$$-2(\\mathbf H\\mathbf{\\bar P})^\\mathsf T + 2\\mathbf K(\\mathbf H \\mathbf{\\bar P}\\mathbf H^\\mathsf T + \\mathbf R) = 0 \\\\\n",
    "\\mathbf K(\\mathbf H \\mathbf{\\bar P}\\mathbf H^\\mathsf T + \\mathbf R) = (\\mathbf H\\mathbf{\\bar P})^\\mathsf T \\\\\n",
    "\\mathbf K(\\mathbf H \\mathbf{\\bar P}\\mathbf H^\\mathsf T + \\mathbf R) = \\mathbf{\\bar P}\\mathbf H^\\mathsf T \\\\\n",
    "\\mathbf K= \\mathbf{\\bar P}\\mathbf H^\\mathsf T (\\mathbf H \\mathbf{\\bar P}\\mathbf H^\\mathsf T + \\mathbf R)^{-1}\n",
    "$$\n",
    "\n",
    "This derivation is not quite iron clad as I left out an argument about why minimizing the trace minimizes the total error, but I think it suffices for this book. Any of the standard texts will go into greater detail if you need it.\n",
    "\n",
    "## Numeric Integration of Differential Equations\n",
    "\n",
    "We've been exposed to several numerical techniques to solve linear differential equations. These include state-space methods, the Laplace transform, and van Loan's method. \n",
    "\n",
    "These work well for linear ordinary differential equations (ODEs), but do not work well for nonlinear equations. For example, consider trying to predict the position of a rapidly turning car. Cars maneuver by turning the front wheels. This makes them pivot around their rear axle as it moves forward. Therefore the path will be continuously varying and a linear prediction will necessarily produce an incorrect value. If the change in the system is small enough relative to $\\Delta t$ this can often produce adequate results, but that will rarely be the case with the nonlinear Kalman filters we will be studying in subsequent chapters. \n",
    "\n",
    "For these reasons we need to know how to numerically integrate ODEs. This can be a vast topic that requires several books. However, I will cover a few simple techniques which will work for a majority of the problems you encounter.\n",
    "\n",
    "\n",
    "### Euler's Method\n",
    "\n",
    "Let's say we have the initial condition problem of \n",
    "\n",
    "$$\\begin{gathered}\n",
    "y' = y, \\\\ y(0) = 1\n",
    "\\end{gathered}$$\n",
    "\n",
    "We happen to know the exact answer is $y=e^t$ because we solved it earlier, but for an arbitrary ODE we will not know the exact solution. In general all we know is the derivative of the equation, which is equal to the slope. We also know the initial value: at $t=0$, $y=1$. If we know these two pieces of information we can predict the value at $y(t=1)$ using the slope at $t=0$ and the value of $y(0)$. I've plotted this below.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "t = np.linspace(-1, 1, 10)\n",
    "plt.plot(t, np.exp(t))\n",
    "t = np.linspace(-1, 1, 2)\n",
    "plt.plot(t,t+1, ls='--', c='k');\n",
    "\n",
    "You can see that the slope is very close to the curve at $t=0.1$, but far from it\n",
    "at $t=1$. But let's continue with a step size of 1 for a moment. We can see that at $t=1$ the estimated value of $y$ is 2. Now we can compute the value at $t=2$ by taking the slope of the curve at $t=1$ and adding it to our initial estimate. The slope is computed with $y'=y$, so the slope is 2.\n",
    "\n",
    "import kf_book.book_plots as book_plots\n",
    "\n",
    "t = np.linspace(-1, 2, 20)\n",
    "plt.plot(t, np.exp(t))\n",
    "t = np.linspace(0, 1, 2)\n",
    "plt.plot([1, 2, 4], ls='--', c='k')\n",
    "book_plots.set_labels(x='x', y='y');\n",
    "\n",
    "Here we see the next estimate for y is 4. The errors are getting large quickly, and you might be unimpressed. But 1 is a very large step size. Let's put this algorithm in code, and verify that it works by using a small step size.\n",
    "\n",
    "def euler(t, tmax, y, dx, step=1.):\n",
    "    ys = []\n",
    "    while t < tmax:\n",
    "        y = y + step*dx(t, y)\n",
    "        ys.append(y)\n",
    "        t +=step        \n",
    "    return ys\n",
    "\n",
    "def dx(t, y): return y\n",
    "\n",
    "print(euler(0, 1, 1, dx, step=1.)[-1])\n",
    "print(euler(0, 2, 1, dx, step=1.)[-1])\n",
    "\n",
    "This looks correct. So now let's plot the result of a much smaller step size.\n",
    "\n",
    "ys = euler(0, 4, 1, dx, step=0.00001)\n",
    "plt.subplot(1,2,1)\n",
    "plt.title('Computed')\n",
    "plt.plot(np.linspace(0, 4, len(ys)),ys)\n",
    "plt.subplot(1,2,2)\n",
    "t = np.linspace(0, 4, 20)\n",
    "plt.title('Exact')\n",
    "plt.plot(t, np.exp(t));\n",
    "\n",
    "print('exact answer=', np.exp(4))\n",
    "print('euler answer=', ys[-1])\n",
    "print('difference =', np.exp(4) - ys[-1])\n",
    "print('iterations =', len(ys))\n",
    "\n",
    "Here we see that the error is reasonably small, but it took a very large number of iterations to get three digits of precision. In practice Euler's method is too slow for most problems, and we use more sophisticated methods.\n",
    "\n",
    "Before we go on, let's formally derive Euler's method, as it is the basis for the more advanced Runge Kutta methods used in the next section. In fact, Euler's method is the simplest form of Runge Kutta.\n",
    "\n",
    "\n",
    "Here are the first 3 terms of the Taylor expansion of $y$. An infinite expansion would give an exact answer, so $O(h^4)$ denotes the error due to the finite expansion.\n",
    "\n",
    "$$y(t_0 + h) = y(t_0) + h y'(t_0) + \\frac{1}{2!}h^2 y''(t_0) + \\frac{1}{3!}h^3 y'''(t_0) +  O(h^4)$$\n",
    "\n",
    "Here we can see that Euler's method is using the first two terms of the Taylor expansion. Each subsequent term is smaller than the previous terms, so we are assured that the estimate will not be too far off from the correct value. \n",
    "\n",
    "### Runge Kutta Methods\n",
    "\n",
    "\n",
    "Runge Kutta is the workhorse of numerical integration. There are a vast number of methods in the literature. In practice, using the Runge Kutta algorithm that I present here will solve most any problem you will face. It offers a very good balance of speed, precision, and stability, and it is the 'go to' numerical integration method unless you have a very good reason to choose something different.\n",
    "\n",
    "Let's dive in. We start with some differential equation\n",
    "\n",
    "$$\\ddot{y} = \\frac{d}{dt}\\dot{y}$$.\n",
    "\n",
    "We can substitute the derivative of y with a function f, like so\n",
    "\n",
    "$$\\ddot{y} = \\frac{d}{dt}f(y,t)$$.\n",
    "\n",
    "Deriving these equations is outside the scope of this book, but the Runge Kutta RK4 method is defined with these equations.\n",
    "\n",
    "$$y(t+\\Delta t) = y(t) + \\frac{1}{6}(k_1 + 2k_2 + 2k_3 + k_4) + O(\\Delta t^4)$$\n",
    "\n",
    "$$\\begin{aligned}\n",
    "k_1 &= f(y,t)\\Delta t \\\\\n",
    "k_2 &= f(y+\\frac{1}{2}k_1, t+\\frac{1}{2}\\Delta t)\\Delta t \\\\\n",
    "k_3 &= f(y+\\frac{1}{2}k_2, t+\\frac{1}{2}\\Delta t)\\Delta t \\\\\n",
    "k_4 &= f(y+k_3, t+\\Delta t)\\Delta t\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Here is the corresponding code:\n",
    "\n",
    "def runge_kutta4(y, x, dx, f):\n",
    "    \"\"\"computes 4th order Runge-Kutta for dy/dx.\n",
    "    y is the initial value for y\n",
    "    x is the initial value for x\n",
    "    dx is the difference in x (e.g. the time step)\n",
    "    f is a callable function (y, x) that you supply \n",
    "    to compute dy/dx for the specified values.\n",
    "    \"\"\"\n",
    "    \n",
    "    k1 = dx * f(y, x)\n",
    "    k2 = dx * f(y + 0.5*k1, x + 0.5*dx)\n",
    "    k3 = dx * f(y + 0.5*k2, x + 0.5*dx)\n",
    "    k4 = dx * f(y + k3, x + dx)\n",
    "    \n",
    "    return y + (k1 + 2*k2 + 2*k3 + k4) / 6.\n",
    "\n",
    "Let's use this for a simple example. Let\n",
    "\n",
    "$$\\dot{y} = t\\sqrt{y(t)}$$\n",
    "\n",
    "with the initial values\n",
    "\n",
    "$$\\begin{aligned}t_0 &= 0\\\\y_0 &= y(t_0) = 1\\end{aligned}$$\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "t = 0.\n",
    "y = 1.\n",
    "dt = .1\n",
    "\n",
    "ys, ts = [], []\n",
    "\n",
    "def func(y,t):\n",
    "    return t*math.sqrt(y)\n",
    "\n",
    "while t <= 10:\n",
    "    y = runge_kutta4(y, t, dt, func)\n",
    "    t += dt\n",
    "    ys.append(y)\n",
    "    ts.append(t)\n",
    "\n",
    "exact = [(t**2 + 4)**2 / 16. for t in ts]\n",
    "plt.plot(ts, ys)\n",
    "plt.plot(ts, exact)\n",
    "\n",
    "error = np.array(exact) - np.array(ys)\n",
    "print(f\"max error {max(error):.5f}\")\n",
    "\n",
    "## Bayesian Filtering\n",
    "\n",
    "Starting in the Discrete Bayes chapter I used a Bayesian formulation for filtering. Suppose we are tracking an object. We define its *state* at a specific time as its position, velocity, and so on. For example, we might write the state at time $t$ as $\\mathbf x_t = \\begin{bmatrix}x_t &\\dot x_t \\end{bmatrix}^\\mathsf T$. \n",
    "\n",
    "When we take a measurement of the object we are measuring the state or part of it. Sensors are noisy, so the measurement is corrupted with noise. Clearly though, the measurement is determined by the state. That is, a change in state may change the measurement, but a change in measurement will not change the state.\n",
    "\n",
    "In filtering our goal is to compute an optimal estimate for a set of states $\\mathbf x_{0:t}$ from time 0 to time $t$. If we knew $\\mathbf x_{0:t}$ then it would be trivial to compute a set of measurements $\\mathbf z_{0:t}$ corresponding to those states. However, we receive a set of measurements $\\mathbf z_{0:t}$, and want to compute the corresponding states $\\mathbf x_{0:t}$. This is called *statistical inversion* because we are trying to compute the input from the output. \n",
    "\n",
    "Inversion is a difficult problem because there is typically no unique solution. For a given set of states $\\mathbf x_{0:t}$ there is only one possible set of measurements (plus noise), but for a given set of measurements there are many different sets of states that could have led to those measurements. \n",
    "\n",
    "Recall Bayes Theorem:\n",
    "\n",
    "$$P(x \\mid z) = \\frac{P(z \\mid x)P(x)}{P(z)}$$\n",
    "\n",
    "where $P(z \\mid x)$ is the *likelihood* of the measurement $z$, $P(x)$ is the *prior* based on our process model, and $P(z)$ is a normalization constant. $P(x \\mid z)$ is the *posterior*, or the distribution after incorporating the measurement $z$, also called the *evidence*.\n",
    "\n",
    "This is a *statistical inversion* as it goes from $P(z \\mid x)$ to $P(x \\mid z)$. The solution to our filtering problem can be expressed as:\n",
    "\n",
    "$$P(\\mathbf x_{0:t} \\mid \\mathbf z_{0:t}) = \\frac{P(\\mathbf z_{0:t} \\mid \\mathbf x_{0:t})P(\\mathbf x_{0:t})}{P(\\mathbf z_{0:t})}$$\n",
    "\n",
    "That is all well and good until the next measurement $\\mathbf z_{t+1}$ comes in, at which point we need to recompute the entire expression for the range $0:t+1$. \n",
    "\n",
    "In practice this is intractable because we are trying to compute the posterior distribution $P(\\mathbf x_{0:t} \\mid \\mathbf z_{0:t})$ for the state over the full range of time steps. But do we really care about the probability distribution at the third step (say) when we just received the tenth measurement? Not usually. So we relax our requirements and only compute the distributions for the current time step. \n",
    "\n",
    "The first simplification is we describe our process (e.g., the motion model for a moving object) as a *Markov chain*. That is, we say that the current state is solely dependent on the previous state and a transition probability $P(\\mathbf x_k \\mid \\mathbf x_{k-1})$, which is just the probability of going from the last state to the current one. We write:\n",
    "\n",
    "$$\\mathbf x_k \\sim P(\\mathbf x_k \\mid \\mathbf x_{k-1})$$\n",
    "\n",
    "In practice this is extremely reasonable, as many things have the *Markov property*. If you are driving in a parking lot, does your position in the next second depend on whether you pulled off the interstate or were creeping along on a dirt road one minute ago? No. Your position in the next second depends solely on your current position, speed, and control inputs, not on what happened a minute ago. Thus, cars have the Markov property, and we can make this simplification with no loss of precision or generality.\n",
    "\n",
    "The next simplification we make is do define the *measurement model* as depending on the current state $\\mathbf x_k$ with the conditional probability of the measurement given the current state: $P(\\mathbf z_k \\mid \\mathbf x_k)$. We write:\n",
    "\n",
    "$$\\mathbf z_k \\sim P(\\mathbf z_k \\mid \\mathbf x_k)$$\n",
    "\n",
    "We have a recurrence now, so we need an initial condition to terminate it. Therefore we say that the initial distribution is the probablity of the state $\\mathbf x_0$:\n",
    "\n",
    "$$\\mathbf x_0 \\sim P(\\mathbf x_0)$$\n",
    "\n",
    "\n",
    "These terms are plugged into Bayes equation. If we have the state $\\mathbf x_0$ and the first measurement we can estimate $P(\\mathbf x_1 | \\mathbf z_1)$. The motion model creates the prior $P(\\mathbf x_2 \\mid \\mathbf x_1)$. We feed this back into Bayes theorem to compute $P(\\mathbf x_2 | \\mathbf z_2)$. We continue this predictor-corrector algorithm, recursively computing the state and distribution at time $t$ based solely on the state and distribution at time $t-1$ and the measurement at time $t$.\n",
    "\n",
    "The details of the mathematics for this computation varies based on the problem. The **Discrete Bayes** and **Univariate Kalman Filter** chapters gave two different formulations which you should have been able to reason through. The univariate Kalman filter assumes that for a scalar state both the noise and process are linear model are affected by zero-mean, uncorrelated Gaussian noise. \n",
    "\n",
    "The Multivariate Kalman filter makes the same assumption but for states and measurements that are vectors, not scalars. Dr. Kalman was able to prove that if these assumptions hold true then the Kalman filter is *optimal* in a least squares sense. Colloquially this means there is no way to derive more information from the noisy measurements. In the remainder of the book I will present filters that relax the constraints on linearity and Gaussian noise.\n",
    "\n",
    "Before I go on, a few more words about statistical inversion. As Calvetti and Somersalo write in *Introduction to Bayesian Scientific Computing*, \"we adopt the Bayesian point of view: *randomness simply means lack of information*\"[3]. Our state parameterizes physical phenomena that we could in principle measure or compute: velocity, air drag, and so on. We lack enough information to compute or measure their value, so we opt to consider them as random variables. Strictly speaking they are not random, thus this is a subjective position. \n",
    "\n",
    "They devote a full chapter to this topic. I can spare a paragraph. Bayesian filters are possible because we ascribe statistical properties to unknown parameters. In the case of the Kalman filter we have closed-form solutions to find an optimal estimate. Other filters, such as the discrete Bayes filter or the particle filter which we cover in a later chapter, model the probability in a more ad-hoc, non-optimal manner. The power of our technique comes from treating lack of information as a random variable, describing that random variable as a probability distribution, and then using Bayes Theorem to solve the statistical inference problem.\n",
    "\n",
    "## Converting Kalman Filter to a g-h Filter\n",
    "\n",
    "I've stated that the Kalman filter is a form of the g-h filter. It just takes some algebra to prove it. It's more straightforward to do with the one dimensional case, so I will do that. Recall \n",
    "\n",
    "$$\n",
    "\\mu_{x}=\\frac{\\sigma_1^2 \\mu_2 + \\sigma_2^2 \\mu_1} {\\sigma_1^2 + \\sigma_2^2}\n",
    "$$\n",
    "\n",
    "which I will make more friendly for our eyes as:\n",
    "\n",
    "$$\n",
    "\\mu_{x}=\\frac{ya + xb} {a+b}\n",
    "$$\n",
    "\n",
    "We can easily put this into the g-h form with the following algebra\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mu_{x}&=(x-x) + \\frac{ya + xb} {a+b} \\\\\n",
    "\\mu_{x}&=x-\\frac{a+b}{a+b}x  + \\frac{ya + xb} {a+b} \\\\ \n",
    "\\mu_{x}&=x +\\frac{-x(a+b) + xb+ya}{a+b} \\\\\n",
    "\\mu_{x}&=x+ \\frac{-xa+ya}{a+b}  \\\\\n",
    "\\mu_{x}&=x+ \\frac{a}{a+b}(y-x)\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We are almost done, but recall that the variance of estimate is given by \n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\sigma_{x}^2 &= \\frac{1}{\\frac{1}{\\sigma_1^2} +  \\frac{1}{\\sigma_2^2}} \\\\\n",
    "&= \\frac{1}{\\frac{1}{a} +  \\frac{1}{b}}\n",
    "\\end{aligned}$$\n",
    "\n",
    "We can incorporate that term into our equation above by observing that\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "\\frac{a}{a+b} &= \\frac{a/a}{(a+b)/a} = \\frac{1}{(a+b)/a}  \\\\\n",
    " &= \\frac{1}{1 + \\frac{b}{a}} = \\frac{1}{\\frac{b}{b} + \\frac{b}{a}}  \\\\\n",
    " &= \\frac{1}{b}\\frac{1}{\\frac{1}{b} + \\frac{1}{a}} \\\\\n",
    " &= \\frac{\\sigma^2_{x}}{b}\n",
    " \\end{aligned}\n",
    "$$\n",
    "\n",
    "We can tie all of this together with\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mu_{x}&=x+ \\frac{a}{a+b}(y-x) \\\\\n",
    "&= x + \\frac{\\sigma^2_{x}}{b}(y-x) \\\\\n",
    "&= x + g_n(y-x)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$g_n = \\frac{\\sigma^2_{x}}{\\sigma^2_{y}}$$\n",
    "\n",
    "The end result is multiplying the residual of the two measurements by a constant and adding to our previous value, which is the $g$ equation for the g-h filter. $g$ is the variance of the new estimate divided by the variance of the measurement. Of course in this case $g$ is not a constant as it varies with each time step as the variance changes. We can also derive the formula for $h$ in the same way. It is not a particularly illuminating derivation and I will skip it. The end result is\n",
    "\n",
    "$$h_n = \\frac{COV (x,\\dot x)}{\\sigma^2_{y}}$$\n",
    "\n",
    "The takeaway point is that $g$ and $h$ are specified fully by the variance and covariances of the measurement and predictions at time $n$. In other words, we are picking a point between the measurement and prediction by a scale factor determined by the quality of each of those two inputs.\n",
    "\n",
    "## References\n",
    "\n",
    " * [1] C.B. Molwer and C.F. Van Loan \"Nineteen Dubious Ways to Compute the Exponential of a Matrix, Twenty-Five Years Later,\", *SIAM Review 45, 3-49*. 2003.\n",
    "\n",
    "\n",
    " * [2] C.F. van Loan, \"Computing Integrals Involving the Matrix Exponential,\" IEEE *Transactions Automatic Control*, June 1978.\n",
    " \n",
    " \n",
    " * [3] Calvetti, D and Somersalo E, \"Introduction to Bayesian Scientific Computing: Ten Lectures on Subjective Computing,\", *Springer*, 2007.\n",
    " \n",
    " * [4] Brown, R. G. and Hwang, P. Y.C., \"Introduction to Random Signals and Applied Kalman Filtering\", *Wiley and Sons*, Fourth Edition, p.143-147, 2012. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
